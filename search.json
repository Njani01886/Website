[
  {
    "objectID": "about/Nayan.html",
    "href": "about/Nayan.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "Education\nUniversity of Massachusetts Amherst, College of Social and Behavioral Sciences, Amherst, MA. Exp. Graduation:: Jan 2024 Masters Concentration: Data Analytics and Computational Social Science Current GPA: 3.89 Relevant Courses: Advanced Quantitative Methods, Regression Models, Text as Data, Advanced Data-Driven Storytelling\nUniversity of Rhode Island, College of Arts and Sciences, Kingston, RI. Graduated:: May 2022 Bachelor of Science Concentration: Data Science Final GPA: 3.52, Dean’s List: 2018, 2019, 2020, 2021, 2022 Relevant Courses: Machine Learning, Multivariate Statistical Learning, Big Data Analysis, Database Management\n\n\nSkills\nGeneral Skills: Data Analysis, Machine Learning, Data Visualization, Data Storytelling, Data Cleaning, NLP\nProgramming Languages: Python, R, and SQL\nLibraries: Ggplot2, Tidyverse, Summarytools, Stats, Tidytext, Quanteda, Rselenium, Lubridate, Devtools, SciPy, Matplotlib, Seaborn, Scikit-learn, Numpy, Pandas\n\n\nProjects\nIdentifying Sources of Poor Nutrition for Americans, Amherst, MA July-August 2023 Project Owner - Analyzed the impact of food sources on food and nutrient intakes for different ages and income levels using 2017-18 Food and\nNutrient Density by Food Source and Demographic Characteristics datasets - Created multiple charts that visualized the 3 way associations between food source, demographics and food/nutrient density using ggplot2 in R - Discovered that adults and seniors were consuming more cholesterol at restaurants than at home, low and middle income individuals had lower intakes of protein at home than away from home - Cleaned and combined datasets using the tidyverse package in R in order to produce visualizations -Explained methods and results using language that a non-technical audience could understand in a 29 page report\nLink: https://github.com/Njani01886/Website/blob/main/projects/Identifying_Sources_of_Poor_Nutrition.pdf\nFixing Social Media Design Brief February-May 2023 Team Member - Developed a design brief for an app called “Social Media Royale” that helps reduce people’s screen time with 2 student colleagues - Conducted background research and interviews over Zoom to understand the consequences of high amounts of screen time - Brainstormed many design ideas that were is used in the final pitch with my team members - Gained substantial knowledge about the process of product design\nLink: https://github.com/Njani01886/Website/blob/main/projects/Slide%20for%20SMR.pdf\nResearch Design Final Project February-May 2023 Team Member - Conducted a survey experiment research study with the goal of understanding if exposure to misinformation increases belief in conspiracy theories with two student colleagues - Defined constructs, operational definitions, ways of measurement for constructs, and treatment groups for the research study - Used Qualtrics for distributing the survey - Analyzed results from the survey using stacked bar charts, One-Way Anova, and Linear Regression using R\nLink: https://github.com/Njani01886/Website/blob/main/projects/602_Poster.pptx.pdf\nNBA Salary Prediction, Amherst, MA February-May 2023 Project Owner - Developed and tested 4 machine learning models using different regression methods to explore which one performs the best at predicting NBA players’ salaries using R and Python - Found that Random Forest performed the best out of all methods, yielding a low RMSLE of 0.50 - Pre-processed and wrangled data into a suitable format for the ML models - Tuned hyperparameters for each model using GridSearchCV with 5 folds to control for overfitting - Presented my work at my program’s research symposium in front of faculty and peers\nLink: https://github.com/Njani01886/Website/blob/main/projects/ML%20final%20poster.pdf\nRegression Models Final Project, Amherst, MA November-December 2022\nTeam Member - Investigated the difference in total goals across Europe’s top 5 soccer leagues using team data from the 2021-22 season to understand if the type of league determines the number of goals per season - Developed a Quasi-Poisson regression model with 9 predictors using R with 3 student colleagues - Concluded that there was a significant difference in goals between Bundesliga and other leagues - Handled the the selection of predictors by creating and analyzing a scatter-plot matrix using GGally - Coordinated weekly group discussions with team members on Zoom and in person\nLink: https://github.com/Njani01886/Website/blob/main/projects/Soccer_Regression.pdf\nAssessing Sentiment Surrounding the 2022 World Cup, Amherst, MA September-December 2022 Project Owner - Analyzed controversies of the Qatar World Cup by conducting Sentiment Analysis, LDA Topic Modeling, Semantic Network, and Pairwise Correlation Analysis on Youtube comments using R - Concluded that the overall sentiment of the comments was negative, main focus of discussion surrounded human rights violations - Used packages tidyverse, quanteda, tidytext, text2vec, LDAvis, and sentimentr to conduct analysis - Extracted 1,391 comments in total from 9 Youtube videos using Youtube API in Python\nHere I will provide two links: One for my code and one for my final poster. Note: The analyses I made in my code post were not my final conclusions. My final conclusions were made on my final poster.\nCode: (https://dacss.github.io/Text_as_Data_Fall_2022/posts/BlogPost6_NayanJani.html) Final Poster: https://github.com/Njani01886/Website/blob/main/projects/World_Cup_Text_Analysis.png\nSpring 2022 Machine Learning Project at URI, Kingston, RI Research Assistant - Designed a program in Python that estimated heterogeneous treatment effects using datasets that involve AIDS and breast cancer treatments as a part of my senior dissertation - Implemented different meta learners in my program that could estimate the conditional average treatment effect to identify how to personalize treatment regimes - Gained experience and knowledge in Causal Inference - Discussed methodology weekly with research leader\n\n\nExperience\nSummer 2021 Bora, La Jolla, CA Business Development & Land Acquisition - Compiled data about potential locations using Google Maps - Analyzed aggregated data to determine which locations are the busiest based on area busyness - Company’s goal is to bring to market an app based, self service beach chair rental service stationed at the most popular locations across the country - Part of outreach campaign to early stage VC and angel investors\nSpring 2018 Westford Academy Spring Recreational Basketball, Westford, MA Youth Basketball Coach/League Co-Director - Organized and operated a Spring League consisting of 80 players with 3 student colleagues as part of my senior year internship - Successfully coordinated a three month league for the Westford Youth Association. All surplus proceeds were donated back to the Westford Academy Girls Basketball Program - Established online sign-up and jersey purchases - Coordinated facilities and scheduling of refereeing staff\n\n\nHometown\nWestford, MA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "I am currently a Data Analytics and Computational Social Science Masters student at UMass Amherst with a passion for solving complex problems that improve business decisions and lives of other people. I have always been intrigued by the power of data, and pursuing a Master’s degree in Data Analytics has allowed me to explore this field in depth.\nThroughout my academic and professional journey, I’ve developed strong skills in data analysis, statistics, and machine learning, and I’m always eager to learn new techniques and tools to better serve my clients. I’ve worked on projects ranging from predictive modeling for housing markets to social media sentiment analysis, and I enjoy using my skills to find insights and patterns that can inform decision-making and drive positive change.\nAdditionally, I have completed several projects that required me to collect, clean, and analyze large sets of data, which provided me with hands-on experience with data cleaning and scraping, feature engineering, model selection, and evaluation using tools such as SQL, R, and Python. My previous internship at Bora, a startup company, enabled me to work with a variety of stakeholders, providing me with experience in working collaboratively, which I believe is essential for success.\nAs a person, I’m motivated by a desire to help others. I believe that data can be a powerful force for improving the quality of life of any individual, and I’m committed to using my skills to help businesses and individuals achieve their goals. Whether I’m working on a complex analysis or brainstorming ideas with colleagues, I always strive to be supportive, empathetic, and collaborative, and I enjoy working with people from diverse backgrounds and perspectives. I’m excited to be part of the next generation of data professionals who will make a positive impact in the world.\nIf you’re interested in connecting with me or learning more about my work, please don’t hesitate to reach out."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Nayan Jani",
    "section": "Research interests",
    "text": "Research interests\nThe topic I explored for my Senior recitation is the use of Machine Learning in estimating the heterogeneous treatment effect using datasets that involve AIDS and breast cancer treatments. More specifically, I looked into certain meta learners that can estimate the conditional average treatment effect so that we can personalize treatment regimes. Here at Umass I hope to learn more about how to apply data science to medical treatments for people with illnesses."
  },
  {
    "objectID": "about.html#hometown",
    "href": "about.html#hometown",
    "title": "Nayan Jani",
    "section": "Hometown",
    "text": "Hometown\nWestford, MA"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "Nayan Jani",
    "section": "Hobbies",
    "text": "Hobbies\n\nVideo Games\nFantasy Football\nWatching Sports\nPlaying Cards and Board Games"
  },
  {
    "objectID": "about.html#fun-fact",
    "href": "about.html#fun-fact",
    "title": "Nayan Jani",
    "section": "Fun fact",
    "text": "Fun fact\nI have over 50 cousins that live in the UK!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "Final Project 604\n\n\n\n\n\n\n\nfinal\n\n\nfood\n\n\n\n\nAnalysis of Food Consumption\n\n\n\n\n\n\nAug 7, 2023\n\n\nNayan Jani\n\n\n\n\n\n\n\n\nNBA Salary Prediction\n\n\n\n\n\n\n\nNBA\n\n\nMachine Learning\n\n\n\n\nMachine Learning for the Social Sciences\n\n\n\n\n\n\nJun 12, 2023\n\n\nNayan Jani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/NBA_Salary.html",
    "href": "posts/NBA_Salary.html",
    "title": "NBA Salary Prediction",
    "section": "",
    "text": "Code\nlibrary(alr4)\nlibrary(tinytex)\nlibrary(summarytools)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(aod)\nlibrary(DescTools)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(GGally)\nlibrary(hrbrthemes)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NBA_Salary.html#introduction",
    "href": "posts/NBA_Salary.html#introduction",
    "title": "NBA Salary Prediction",
    "section": "Introduction",
    "text": "Introduction\nBeing a General Manager in the NBA comes with a ton of decisions. One of the most important decisions a GM can make is how much they pay the players on their team. It is so important to pay the players the right amount in order to build the strongest roster. Overpaying a player will hurt a teams cap space, meaning that the team will not be able to sign good players because they do not have enough money to afford them. My motivation for this project its to see if Machine Learning techniques can correctly predict a players salary. The idea is if I am able to create a model that performs well enough, then it could be used as a tool to determine a players salary for their next contract. Here I will perform different regression methods to predict players salary and then used the best method for prediction."
  },
  {
    "objectID": "posts/NBA_Salary.html#the-data",
    "href": "posts/NBA_Salary.html#the-data",
    "title": "NBA Salary Prediction",
    "section": "The Data",
    "text": "The Data\nThe dataset I am using comes from Kaggle. The dataset contains information about player names, time span of the contract, avg salary per year and all stats that player accumulated during NBA season before signing their next contract. The scope of the data is as follows: - There are only contracts signed since 2010/2011 season to 2019/2020 season. - Only includes players that are active in 2020/2021 season. - Doesn’t include rookie or retained contracts. - Doesn’t include contracts for player that haven’t played year before the signing the contract.\nThis is a good scope because I want to use modern players contracts for future predictions. The limitation of only including players that are active in 20/21 means that these players were able to earn multiple contracts of the 10 year span, which validates them as players who are worth to continuing paying. Having this removes players who had massive contracts early in their career and then faded out quickly after their primes.\n\n\nCode\ndf<- read_csv(\"_data/nba_contracts_history.csv\")\n\n\nRows: 199 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (27): CONTRACT_START, CONTRACT_END, AVG_SALARY, AGE, GP, W, L, MIN, PTS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndf\n\n\n# A tibble: 199 × 28\n   NAME  CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n   <chr>   <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Wesl…    2019    2020  2.56e6    32    69    27    42  2091   840   279   698\n 2 Broo…    2015    2017  2.12e7    27    72    34    38  2100  1236   506   987\n 3 DeAn…    2011    2014  1.08e7    22    80    31    49  2047   566   234   341\n 4 Mark…    2015    2018  8.14e6    25    82    39    43  2581  1258   512  1100\n 5 Dwig…    2018    2019  1.34e7    32    81    35    46  2463  1347   506   911\n 6 Aust…    2015    2016  7.06e6    22    76    46    30  1563   530   203   496\n 7 Wayn…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n 8 JaMy…    2019    2020  4.77e6    29    65    31    34  1371   611   230   476\n 9 Kyle…    2015    2018  4.05e6    25    51    15    36   824   294   119   242\n10 Trev…    2014    2017  8   e6    28    77    41    36  2723  1107   389   853\n# … with 189 more rows, 16 more variables: `FG%` <dbl>, `3PM` <dbl>,\n#   `3PA` <dbl>, `3P%` <dbl>, FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>,\n#   DREB <dbl>, REB <dbl>, AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>,\n#   PF <dbl>, `+/-` <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\n\n\nCode\ndf<- df %>% mutate(c_duration = CONTRACT_END -CONTRACT_START)\n\n\nAfter loading in the data I can see it has 199 instances with 29 features. Some features will be removed such as player name, CONTRACT_START and CONTRACT_end. After EDA I will remove any features that interfere with my regression analysis. The only variable mutation I did was c_duration, which is how long a players contract lasted."
  },
  {
    "objectID": "posts/NBA_Salary.html#eda",
    "href": "posts/NBA_Salary.html#eda",
    "title": "NBA Salary Prediction",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nggplot(data=df, aes(x=AGE, y=AVG_SALARY)) +\n  geom_bar(stat=\"identity\") +\n  labs(title = \"Average Salary by Age\", x = \"Age\" ,y= \"Average Salary\")\n\n\n\n\n\nThe first relationship I wanted to visualize was Average salary and Age. Age is very important when thinking about how much to pay someone because you want to give them a contract that shows how good they can perform for that length of contract. I can see that average salary peaks at age 23 and again rises at 27 and then 30. This make sense because young players who are really good will receive a massive 2nd contract (contract after rookie deal). The peaks at 27 and 30 could be from players signing their 3rd or 4th contracts but make less money due to their age.\n\n\nCode\nprint(dfSummary(df, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ndf\nDimensions: 199 x 29\n  Duplicates: 1\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      NAME\n[character]\n      1. Kevin Durant2. Austin Rivers3. Avery Bradley4. Danilo Gallinari5. Dwight Howard6. E'Twaun Moore7. Paul Millsap8. Quinn Cook9. Robin Lopez10. Wayne Ellington[ 128 others ]\n      4(2.0%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)168(84.4%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_START\n[numeric]\n      Mean (sd) : 2015.2 (2.1)min ≤ med ≤ max:2011 ≤ 2015 ≤ 2019IQR (CV) : 3 (0)\n      2011:9(4.5%)2012:16(8.0%)2013:16(8.0%)2014:21(10.6%)2015:48(24.1%)2016:35(17.6%)2017:23(11.6%)2018:20(10.1%)2019:11(5.5%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_END\n[numeric]\n      Mean (sd) : 2017.5 (1.7)min ≤ med ≤ max:2013 ≤ 2018 ≤ 2020IQR (CV) : 3 (0)\n      2013:2(1.0%)2014:11(5.5%)2015:18(9.0%)2016:23(11.6%)2017:29(14.6%)2018:46(23.1%)2019:55(27.6%)2020:15(7.5%)\n      \n      0\n(0.0%)\n    \n    \n      AVG_SALARY\n[numeric]\n      Mean (sd) : 11073609 (7897820)min ≤ med ≤ max:823244 ≤ 9500000 ≤ 33599500IQR (CV) : 11621898 (0.7)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AGE\n[numeric]\n      Mean (sd) : 25.9 (2.8)min ≤ med ≤ max:20 ≤ 25 ≤ 36IQR (CV) : 4 (0.1)\n      16 distinct values\n      \n      0\n(0.0%)\n    \n    \n      GP\n[numeric]\n      Mean (sd) : 64.2 (19.6)min ≤ med ≤ max:1 ≤ 72 ≤ 82IQR (CV) : 19 (0.3)\n      59 distinct values\n      \n      0\n(0.0%)\n    \n    \n      W\n[numeric]\n      Mean (sd) : 34.2 (14.5)min ≤ med ≤ max:0 ≤ 35 ≤ 64IQR (CV) : 20 (0.4)\n      56 distinct values\n      \n      0\n(0.0%)\n    \n    \n      L\n[numeric]\n      Mean (sd) : 30 (13)min ≤ med ≤ max:0 ≤ 31 ≤ 62IQR (CV) : 18.5 (0.4)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      MIN\n[numeric]\n      Mean (sd) : 1747 (782.4)min ≤ med ≤ max:2 ≤ 1867 ≤ 3125IQR (CV) : 1125.5 (0.4)\n      193 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PTS\n[numeric]\n      Mean (sd) : 813.4 (499.9)min ≤ med ≤ max:0 ≤ 734 ≤ 2376IQR (CV) : 710.5 (0.6)\n      186 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGM\n[numeric]\n      Mean (sd) : 300.4 (178.6)min ≤ med ≤ max:0 ≤ 277 ≤ 743IQR (CV) : 261 (0.6)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGA\n[numeric]\n      Mean (sd) : 638.8 (374.6)min ≤ med ≤ max:1 ≤ 572 ≤ 1643IQR (CV) : 553.5 (0.6)\n      189 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FG%\n[numeric]\n      Mean (sd) : 46.7 (8.1)min ≤ med ≤ max:0 ≤ 45.5 ≤ 100IQR (CV) : 7 (0.2)\n      131 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PM\n[numeric]\n      Mean (sd) : 63.6 (58.4)min ≤ med ≤ max:0 ≤ 57 ≤ 272IQR (CV) : 91 (0.9)\n      111 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PA\n[numeric]\n      Mean (sd) : 173.9 (148.7)min ≤ med ≤ max:0 ≤ 170 ≤ 657IQR (CV) : 241 (0.9)\n      139 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3P%\n[numeric]\n      Mean (sd) : 29.7 (13.2)min ≤ med ≤ max:0 ≤ 34.8 ≤ 50IQR (CV) : 11.9 (0.4)\n      110 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTM\n[numeric]\n      Mean (sd) : 149 (128.4)min ≤ med ≤ max:0 ≤ 107 ≤ 720IQR (CV) : 143 (0.9)\n      143 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTA\n[numeric]\n      Mean (sd) : 195.5 (162.2)min ≤ med ≤ max:0 ≤ 145 ≤ 837IQR (CV) : 192.5 (0.8)\n      162 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FT%\n[numeric]\n      Mean (sd) : 74 (15)min ≤ med ≤ max:0 ≤ 76.8 ≤ 100IQR (CV) : 11.4 (0.2)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OREB\n[numeric]\n      Mean (sd) : 79.3 (72.7)min ≤ med ≤ max:0 ≤ 51 ≤ 397IQR (CV) : 85.5 (0.9)\n      117 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DREB\n[numeric]\n      Mean (sd) : 249 (164)min ≤ med ≤ max:1 ≤ 209 ≤ 829IQR (CV) : 235 (0.7)\n      167 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REB\n[numeric]\n      Mean (sd) : 328.3 (226.1)min ≤ med ≤ max:1 ≤ 286 ≤ 1226IQR (CV) : 305 (0.7)\n      169 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AST\n[numeric]\n      Mean (sd) : 171.7 (163.9)min ≤ med ≤ max:0 ≤ 110 ≤ 839IQR (CV) : 137 (1)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      TOV\n[numeric]\n      Mean (sd) : 103.4 (70.9)min ≤ med ≤ max:0 ≤ 85 ≤ 374IQR (CV) : 83 (0.7)\n      138 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STL\n[numeric]\n      Mean (sd) : 58.2 (37.3)min ≤ med ≤ max:0 ≤ 48 ≤ 169IQR (CV) : 43 (0.6)\n      100 distinct values\n      \n      0\n(0.0%)\n    \n    \n      BLK\n[numeric]\n      Mean (sd) : 39.6 (43.3)min ≤ med ≤ max:0 ≤ 23 ≤ 269IQR (CV) : 40 (1.1)\n      83 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PF\n[numeric]\n      Mean (sd) : 137.7 (64.3)min ≤ med ≤ max:1 ≤ 137 ≤ 291IQR (CV) : 84 (0.5)\n      135 distinct values\n      \n      0\n(0.0%)\n    \n    \n      +/-\n[numeric]\n      Mean (sd) : 62.9 (227.7)min ≤ med ≤ max:-628 ≤ 44 ≤ 839IQR (CV) : 257.5 (3.6)\n      171 distinct values\n      \n      0\n(0.0%)\n    \n    \n      c_duration\n[numeric]\n      Mean (sd) : 2.3 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.5)\n      1:69(34.7%)2:36(18.1%)3:66(33.2%)4:28(14.1%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.0)2023-06-12\n\n\n\nHere I created summary statistics of the dataset using SummaryTools. The two stats that stood out to me was the mean games played and mean minutes played. The GP and MIN mean values are 64.2 and 1747, respectively. This implies that in order to be considered for another contract, players must play most of the season and play about 21 minutes per game. I would say that if you looked at all the means and medians of each feature, then those values represent a player who will get another NBA contract.\n\n\nCode\nggplot(data = df, mapping = aes(x = MIN, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Minutes Played\", x = \"Mins\" ,y= \"Average Salary\")\n\n\n\n\n\nNext I wanted to check the relationship between Minutes played and Average Salary. I can see that their relationship is non-linear but positive. I see an increase of salary once a player is playing roughly 1750 minutes but drops off around 2500 minutes.The positive relationship could suggest that minutes played can be a good predictor.\n\n\nCode\nggplot(data = df, mapping = aes(x = `+/-`, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Plus/Minus\", x = \"+/-\" ,y= \"Average Salary\")\n\n\n\n\n\nHere I wanted to see the relationship between +/- and Average salary. +/- is a sports statistic used to measure a player’s impact on the game, represented by the difference between their team’s total scoring versus their opponent’s when the player is in the game. I can see the relationship between +/- and Average Salary is non linear. I can see that there are players with awful +/- that are getting payed more than players with high +/-. I also see a lot of data grouped around 0 with low salaries, meaning these players probably did not play much.\n\n\nCode\ndf %>% filter(`+/-` < -300)\n\n\n# A tibble: 7 × 29\n  NAME   CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n  <chr>    <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Wayne…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n2 JaVal…    2012    2015  1.13e7    24    61    22    39  1535   691   307   552\n3 Gordo…    2014    2017  1.90e7    24    77    23    54  2800  1248   426  1032\n4 Jorda…    2016    2019  1.25e7    24    79    17    62  2552  1225   475  1098\n5 Derri…    2014    2017  1.2 e7    22    73    25    48  2201   970   390   747\n6 JaKar…    2015    2016  1.10e6    22    74    17    57  1131   386   146   346\n7 Nikol…    2015    2018  1.2 e7    24    74    21    53  2529  1428   631  1206\n# … with 17 more variables: `FG%` <dbl>, `3PM` <dbl>, `3PA` <dbl>, `3P%` <dbl>,\n#   FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>, DREB <dbl>, REB <dbl>,\n#   AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>, PF <dbl>, `+/-` <dbl>,\n#   c_duration <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\nHere I investigate the players with terrible +/-. Some of these players are getting paid well but their teams are so bad that their +/- statistic is negative. This makes me believe that non-linear regression methods would be useful if this feature is used for prediction of salary.\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 2:7),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 8:13),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 22:26),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 14:20),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\nlibrary(ggcorrplot)\n\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  cor() %>% \n  ggcorrplot(hc.order = TRUE, type = \"lower\",outline.col = \"white\",lab=TRUE, lab_size=1)\n\n\n\n\n\nHere I created 3 ggpairs plots and a correlation matrix to investigate the correlations between my target variable and and features. I am looking for strong and weak correlations. Based on the correlation matrix, I see a mixture or strong and weak correlations between features and my target variable, Average Salary. This makes me believe that non linear regression methods will perform better than linear regression methods on this dataset.\n\n\nCode\ndf %>%  ggplot(aes(x=AVG_SALARY)) + \n  geom_density() +\n  geom_vline(aes(xintercept=mean(AVG_SALARY)),\n            color=\"blue\", linetype=\"dashed\", size=1)\n\n\n\n\n\nCode\ndf %>% dplyr::select(AVG_SALARY) %>% \n  summarise(mean = mean(AVG_SALARY))\n\n\n# A tibble: 1 × 1\n       mean\n      <dbl>\n1 11073609.\n\n\nHere I wanted to look at the distribution of my target variable. I can see the distribution is not normal and skewed right. Seeing this makes me think that more flexible methods will work better when I create my regression models."
  },
  {
    "objectID": "posts/NBA_Salary.html#pre-processing-in-python",
    "href": "posts/NBA_Salary.html#pre-processing-in-python",
    "title": "NBA Salary Prediction",
    "section": "Pre Processing in Python",
    "text": "Pre Processing in Python\n\n\nCode\ndf = pd.read_csv(\"_data/nba_contracts_history.csv\")\n\n\n\n\nCode\n#Creating variable c_duration\ndf[\"c_duration\"] = df[\"CONTRACT_END\"] - df[\"CONTRACT_START\"] \n\n\n\n\nCode\n#Creates set of features I will use\nfeatures = df.drop([\"NAME\",\"CONTRACT_START\", \"CONTRACT_END\",\"AVG_SALARY\",\"W\",\"L\"], axis=1)\n\n\n\n\nCode\n#sub-setting my target variable, Average Salary\ntarget = df['AVG_SALARY'].to_numpy()\n\n\nHere is my data splits, 70% training, 15% validation and 15% Test.\n\n\nCode\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.3, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state= 42)\n\n\nHere I check to see if the data is split up correctly.\n\n\nCode\nprint(X_train.shape, X_valid.shape, X_test.shape)\n\n\n(139, 23) (30, 23) (30, 23)\n\n\nCode\nprint(Y_train.shape, Y_valid.shape, Y_test.shape)\n\n\n(139,) (30,) (30,)\n\n\nHere I use the make_scorer() function so that I can call RMSLE during Grid Search Cross Validation.\n\n\nCode\nRMSLE = make_scorer(mean_squared_log_error, squared=False)"
  },
  {
    "objectID": "posts/NBA_Salary.html#random-forest",
    "href": "posts/NBA_Salary.html#random-forest",
    "title": "NBA Salary Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nFor hyperparameter tuning, I chose to tune n_estimators, max_features, min_samples_split and min_sample_leafs. From Sckit learn’s documentation, it states that n_estimators and max_features are the main parameters to tune, so I will follow the documentations suggestion. I included min_sample_split and min_sample_leafs because I want to control my model for overfitting. By increasing the values of these two parameters, it will help prevent my model from overfitting.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\n\nparam_grid_RF = {\n    'n_estimators': [100, 200, 300, 400],\n     'max_features': [None, 1.0],\n     'min_samples_split': [2, 4, 6 ,8],\n    'min_samples_leaf': [1, 2 ,4, 6, 8]}\n    \n    \n    \ngridRF= GridSearchCV(RF, param_grid_RF,scoring = RMSLE, cv=5)\ngridRF.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [None, 1.0],\n                         'min_samples_leaf': [1, 2, 4, 6, 8],\n                         'min_samples_split': [2, 4, 6, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridRF.best_params_)\n\n\nBest parameters: {'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRF.best_score_)\n\n\nTraining error (RMSLE): 0.5703407520704454\n\n\nChecking Validation Error for Random Forests.\n\n\nCode\n\nbest_model_RF = gridRF.best_estimator_\npredict_y_RF = best_model_RF.predict(X_valid)\n\nRF_rmsle = mean_squared_log_error(Y_valid, predict_y_RF,squared=False)\n\n\nprint(\"Validation error (RMSLE):\",RF_rmsle)\n\n\nValidation error (RMSLE): 0.45497526641066893"
  },
  {
    "objectID": "posts/NBA_Salary.html#scaling-data",
    "href": "posts/NBA_Salary.html#scaling-data",
    "title": "NBA Salary Prediction",
    "section": "Scaling Data",
    "text": "Scaling Data\nScaling data for SVM and Ridge Regression.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/NBA_Salary.html#svr",
    "href": "posts/NBA_Salary.html#svr",
    "title": "NBA Salary Prediction",
    "section": "SVR",
    "text": "SVR\nThe hyperparameters I tuned from SVR are C, gamma and kernel. I needed to tune for Kernel so that my model finds the best hyper plane that fits the datapoints from my target variable. The kernel will deal with the non-linear relationship on between my features and target variable. Tuning for C and gamma will help my model from overfitting.\n\n\nCode\nfrom sklearn.svm import SVR\n\nSVR = SVR()\n\nparam_grid_SVR = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [0.0001, 0.001,0.01, 0.1, 1],\n              'kernel': ['rbf', 'poly']}\ngridSVR= GridSearchCV(SVR, param_grid_SVR, scoring = RMSLE, cv=5)\ngridSVR.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=SVR(),\n             param_grid={'C': [0.1, 1, 10, 100, 1000],\n                         'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'kernel': ['rbf', 'poly']},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridSVR.best_params_)\n\n\nBest parameters: {'C': 0.1, 'gamma': 0.0001, 'kernel': 'poly'}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridSVR.best_score_)\n\n\nTraining error (RMSLE): 1.0135970157480874\n\n\nChecking Validation Error for SVR.\n\n\nCode\nbest_model_SVR = gridSVR.best_estimator_\npredict_y_SVR = best_model_SVR.predict(X_valid_scaled)\n\nSVR_rmsle = mean_squared_log_error(Y_valid, predict_y_SVR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", SVR_rmsle)\n\n\nValidation error (RMSLE): 0.6805718407616901"
  },
  {
    "objectID": "posts/NBA_Salary.html#ridge",
    "href": "posts/NBA_Salary.html#ridge",
    "title": "NBA Salary Prediction",
    "section": "Ridge",
    "text": "Ridge\nFor Ridge Regression, The only hyperparameter I tuned was alpha, which is the penalty term. I chose to search through small, intermediate, and large values of alpha.\n\n\nCode\nridge = Ridge()\nparam_grid_ridge = {'alpha':np.concatenate((np.arange(0.1,2,0.1), np.arange(2, 5, 0.5), np.arange(5, 105, 5)))}\ngridRidge = GridSearchCV(ridge, param_grid_ridge,scoring = RMSLE, cv=5)\ngridRidge.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=Ridge(),\n             param_grid={'alpha': array([  0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.8,   0.9,\n         1. ,   1.1,   1.2,   1.3,   1.4,   1.5,   1.6,   1.7,   1.8,\n         1.9,   2. ,   2.5,   3. ,   3.5,   4. ,   4.5,   5. ,  10. ,\n        15. ,  20. ,  25. ,  30. ,  35. ,  40. ,  45. ,  50. ,  55. ,\n        60. ,  65. ,  70. ,  75. ,  80. ,  85. ,  90. ,  95. , 100. ])},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridRidge.best_params_)\n\n\nBest parameters: {'alpha': 100.0}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRidge.best_score_)\n\n\nTraining error (RMSLE): 0.5579904916871834\n\n\nChecking Validation Error for Ridge Regression.\n\n\nCode\nbest_model_ridge = gridRidge.best_estimator_\npredict_y_ridge = best_model_ridge.predict(X_valid_scaled)\n\n\nridge_rmsle = mean_squared_log_error(Y_valid, predict_y_ridge,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", ridge_rmsle)\n\n\nValidation error (RMSLE): 0.5200462148165859"
  },
  {
    "objectID": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "href": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "title": "NBA Salary Prediction",
    "section": "Gradient Boosting Decision Trees Regressor",
    "text": "Gradient Boosting Decision Trees Regressor\nThe hyperparameters I chose to tune for the Gradient Boosting Decision Trees Regressor are n_estimators, learning_rate, and max_depth. GBDTR uses multiple shallow decision trees as weak learners to predict the residuals of the decision trees instead of the target variable. The idea here is use gradient descent to update the residuals after every tree until the model has run through all available trees (n_estimators). Learning rate controls how much the residuals are updated from tree to tree, which can also be described as the “size” of the step in gradient descent. Smaller values of learning rate will allow my model to generalize better on the validation and test data. Max_depth refers to the depth of each tree, which is important because it determines how weak the trees are in the ensemble.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\n\nparam_grid_GBR = {\n    'n_estimators': [100, 200, 300, 400],\n     'learning_rate': [ 0.01, 0.1, 0.2,0.3],\n     'max_depth': [3,4,5,6,7,8]}\n    \n    \n    \ngridGBR= GridSearchCV(GBR, param_grid_GBR,scoring = RMSLE, cv=5)\ngridGBR.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                         'max_depth': [3, 4, 5, 6, 7, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridGBR.best_params_)\n\n\nBest parameters: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridGBR.best_score_)\n\n\nTraining error (RMSLE): 0.7298632180107352\n\n\nChecking Validation Error for Gradient Boosting Decision Trees.\n\n\nCode\nbest_model_GBR = gridGBR.best_estimator_\npredict_y_GBR = best_model_GBR.predict(X_valid)\n\n\nGBR_rmsle = mean_squared_log_error(Y_valid, predict_y_GBR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", GBR_rmsle)\n\n\nValidation error (RMSLE): 0.5732402442589972"
  },
  {
    "objectID": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "href": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "title": "NBA Salary Prediction",
    "section": "Final Evaluation using Test Set",
    "text": "Final Evaluation using Test Set\nChecking Test Error for all models. Again I will list the best parameters selected from Grid Search CV.\n\n\nCode\n\nbest_model_RF\n\n\nRandomForestRegressor(max_features=None, min_samples_split=6)\n\n\nCode\npredict_test_RF = best_model_RF.predict(X_test)\n\n\nRF_test_rmsle = mean_squared_log_error(Y_test, predict_test_RF,squared=False)\n\n\nprint(\"Test error (RMSLE):\", RF_test_rmsle)\n\n\nTest error (RMSLE): 0.4986784364298934\n\n\n\n\nCode\nbest_model_SVR \n\n\nSVR(C=0.1, gamma=0.0001, kernel='poly')\n\n\nCode\npredict_test_SVR = best_model_SVR.predict(X_test_scaled)\n\nSVR_test_rmsle = mean_squared_log_error(Y_test, predict_test_SVR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", SVR_test_rmsle)\n\n\nTest error (RMSLE): 1.0137954511265557\n\n\n\n\nCode\nbest_model_ridge\n\n\nRidge(alpha=100.0)\n\n\nCode\npredict_test_ridge = best_model_ridge.predict(X_test_scaled)\n\n\nridge_test_rmsle = mean_squared_log_error(Y_test, predict_test_ridge,squared=False)\n\n\nprint(\"Test error (RMSLE):\", ridge_test_rmsle)\n\n\nTest error (RMSLE): 0.5376660192539188\n\n\n\n\nCode\n\nbest_model_GBR\n\n\nGradientBoostingRegressor(learning_rate=0.01, max_depth=6)\n\n\nCode\npredict_test_GBR = best_model_GBR.predict(X_test)\n\n\nGBR_test_rmsle = mean_squared_log_error(Y_test, predict_test_GBR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", GBR_test_rmsle)\n\n\nTest error (RMSLE): 0.7875342011788629"
  },
  {
    "objectID": "posts/NJ_604.html",
    "href": "posts/NJ_604.html",
    "title": "Final Project 604",
    "section": "",
    "text": "food_density <- read_excel(\"_data/_food/food_density.xlsx\",skip =2 )\n\n\nlookup <- c(Total= \"Total...2\", at_home = \"At home...3\", total_away = \"Total...4\", Restaurant = \"Restaurant...5\", Fast_food = \"Fast food...6\", school  =\"School...7\",other = \"Other...8\", Total1 =\"Total...9\", at_home1 = \"At home...10\", total_away1 = \"Total...11\", Restaurant1 = \"Restaurant...12\",Fast_food1 = \"Fast food...13\",school1  = \"School...14\", other1 =\"Other...15\" )\n\nfood_density<- food_density %>% \n  rename(all_of(lookup)) \n\n\ncolnames(food_density)\n\n [1] \"Food group\"  \"Total\"       \"at_home\"     \"total_away\"  \"Restaurant\" \n [6] \"Fast_food\"   \"school\"      \"other\"       \"Total1\"      \"at_home1\"   \n[11] \"total_away1\" \"Restaurant1\" \"Fast_food1\"  \"school1\"     \"other1\"     \n\n\n\nfood_density<-food_density %>% \n  filter(!row_number() %in% 1)\nfood_density  \n\n\n\n  \n\n\n\n\nfood_density$Restaurant <- as.double(food_density$Restaurant)\nfood_density$Restaurant1 <- as.double(food_density$Restaurant1)\nfood_density$school <- as.double(food_density$school)\nfood_density$school1 <- as.double(food_density$school1)\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"food_source\", values_to = \"values\")\n\n\n\n  \n\n\n\n\nfood_density<- food_density %>% \n  mutate(food_type = \"Added_sugar\")\n\n\nx0<- food_density %>% slice(2:8)\n\n\nx1<- food_density %>% \n    slice(10:16) %>% \n    mutate(food_type = \"Discretionary_fats\" )\n  \nx2<-food_density %>% \n    slice(18:24) %>% \n    mutate(food_type = \"Discretionary_oils\" )\n\nx3<-food_density %>% \n    slice(26:32) %>% \n    mutate(food_type = \"Dairy\")\n\nx4<-food_density %>% \n    slice(34:40) %>% \n    mutate(food_type = \"Fruit\" )\n\nx5<-food_density %>% \n    slice(42:48) %>% \n    mutate(food_type = \"Vegetables_total\" )\n\nx6<-food_density %>% \n    slice(50:56) %>% \n    mutate(food_type = \"Potatoes\" )\n\nx7<-food_density %>% \n    slice(58:64) %>% \n    mutate(food_type = \"Tomatoes\" )\n\nx8<-food_density %>% \n    slice(66:72) %>% \n    mutate(food_type = \"Red_Orange_Vegatables\" )\n\nx9<-food_density %>% \n    slice(74:80) %>% \n    mutate(food_type = \"Dark_Green_Vegatables\" )\n\nx10<-food_density %>% \n    slice(82:88) %>% \n    mutate(food_type = \"Grains\" )\n\nx12<-food_density %>% \n    slice(90:96) %>% \n    mutate(food_type = \"Grains_Refined\")\n\nx13<-food_density %>% \n    slice(98:104) %>% \n    mutate(food_type = \"Grains_Whole\" )\n\nx14<-food_density %>% \n    slice(106:112) %>% \n    mutate(food_type = \"Protein_foods\" )\n\n\nfood_density<- full_join(x0,x1) %>%\n  full_join(x2) %>%\n  full_join(x3) %>%\n  full_join(x4) %>%\n  full_join(x5) %>% \n  full_join(x6) %>% \n  full_join(x7) %>% \n  full_join(x8) %>% \n  full_join(x9) %>% \n  full_join(x10) %>% \n  full_join(x12) %>% \n  full_join(x13) %>%\n  full_join(x14)\n\n\nfood_density <- food_density %>% relocate(food_type, .after = `Food group`)\n\n\nfood_density <-food_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"food_source\", values_to = \"values\")\n\n\nz<- \"2016-2017\"\nfood_density<-food_density %>% \n              mutate(year = case_when(\n              food_source == \"Total1\" ~ \"2017-2018\",\n              food_source == \"at_home1\" ~ \"2017-2018\",\n              food_source == \"total_away1\"~ \"2017-2018\",\n              food_source == \"Restaurant1\"~ \"2017-2018\",\n              food_source == \"Fast_food1\"~ \"2017-2018\",\n              food_source == \"school1\"~ \"2017-2018\",\n              food_source == \"other1\"~ \"2017-2018\",\n              .default = z)) %>% \n              relocate(year, .after = `Food group`)\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_density <-food_density %>% rename(\"Demographics\"= `Food group`)\n\n\nfood_density<-food_density %>% \n  mutate(food_source = str_remove(food_source, \"1\"))\n\n\nfood_density <- food_density %>% \n  mutate(Demographics = str_remove(Demographics, \"1$\"))\nfood_density <- food_density %>% \n  mutate(Demographics = str_remove(Demographics, \"2$\"))\n\n\nfood_density\n\n\n\n  \n\n\n\n\nfood_benchmark <- read_excel(\"_data/_food/food_benchmark.xlsx\",skip =2)\n\n\nfood_benchmark<- food_benchmark %>%\n  filter(!row_number() %in% c(1, 2, 13,14,15))\n\n\nfood_benchmark$`1400.0`<- as.double(food_benchmark$`1400.0`)\n\n\nfood_benchmark <- food_benchmark %>% pivot_longer(c(\"1400.0\", \"2000.0\", \"2200.0\",\"3000.0\" ), names_to = \"Calorie_Intake_Level\", values_to = \"values\")\n\n\nfood_benchmark\n\n\n\n  \n\n\n\n\nfood_nutrient_density <- read_excel(\"_data/_food/food_nutrient_density.xlsx\",skip =2 )\n\n\nfood_nutrient_density<- food_nutrient_density  %>% \n                        rename(all_of(lookup)) \n\n\nfood_nutrient_density<-food_nutrient_density %>% \n  filter(!row_number() %in% 1)\n\n\nfood_nutrient_density$Restaurant <- as.double(food_nutrient_density$Restaurant)\nfood_nutrient_density$Restaurant1 <- as.double(food_nutrient_density$Restaurant1)\nfood_nutrient_density$school <- as.double(food_nutrient_density$school)\nfood_nutrient_density$school1 <- as.double(food_nutrient_density$school1)\n\n\nfood_nutrient_density<- food_nutrient_density %>% \n  mutate(nutrient_type = \"Calcium\")\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nx0<- food_nutrient_density %>% slice(2:8)\n\nx1<- food_nutrient_density %>% \n    slice(10:16) %>% \n    mutate(nutrient_type = \"Cholesterol\")\n  \nx2<-food_nutrient_density %>% \n    slice(18:24) %>% \n    mutate(nutrient_type = \"Fiber\")\n\nx3<-food_nutrient_density %>% \n    slice(26:32) %>% \n    mutate(nutrient_type = \"Iron\")\n\nx4<-food_nutrient_density %>% \n    slice(34:40) %>% \n    mutate(nutrient_type = \"Saturated_fat\" )\n\nx5<-food_nutrient_density %>% \n    slice(42:48) %>% \n    mutate(nutrient_type = \"Total_fat\" )\n\nx6<-food_nutrient_density %>% \n    slice(50:56) %>% \n    mutate(nutrient_type = \"Sodium\" )\n\n\nfood_nutrient_density<- full_join(x0,x1) %>%\n  full_join(x2) %>%\n  full_join(x3) %>%\n  full_join(x4) %>%\n  full_join(x5) %>% \n  full_join(x6)\n\n\nfood_nutrient_density <- food_nutrient_density %>% relocate(nutrient_type, .after = `Nutrient group`)\n\n\nfood_nutrient_density<- food_nutrient_density %>% \n  pivot_longer(c(\"Total\" ,\"at_home\", \"total_away\",\"Restaurant\", \"Fast_food\", \"school\" ,     \"other\" ,\"Total1\"  ,\"at_home1\" ,\"total_away1\", \"Restaurant1\", \"Fast_food1\", \"school1\" ,\"other1\" ), names_to = \"nutrient_source\", values_to = \"values\")\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nfood_nutrient_density<-food_nutrient_density %>% \n              mutate(year = case_when(\n              nutrient_source == \"Total1\" ~ \"2017-2018\",\n              nutrient_source == \"at_home1\" ~ \"2017-2018\",\n              nutrient_source == \"total_away1\"~ \"2017-2018\",\n              nutrient_source == \"Restaurant1\"~ \"2017-2018\",\n              nutrient_source == \"Fast_food1\"~ \"2017-2018\",\n              nutrient_source == \"school1\"~ \"2017-2018\",\n              nutrient_source == \"other1\"~ \"2017-2018\",\n              .default = z)) %>% \n              relocate(year, .after = `Nutrient group`)\n\n\nfood_nutrient_density <-food_nutrient_density %>% rename(\"Demographics\"= `Nutrient group`)\n\nfood_nutrient_density<-food_nutrient_density %>% \n  mutate(nutrient_source = str_remove(nutrient_source, \"1\"))\n\nfood_nutrient_density <- food_nutrient_density %>% \n  mutate(Demographics = str_remove(Demographics, \"1$\"))\nfood_nutrient_density <- food_nutrient_density %>% \n  mutate(Demographics = str_remove(Demographics, \"2$\"))\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\nfood_density\n\n\n\n  \n\n\nfood_benchmark\n\n\n\n  \n\n\n\n\nfood_nutrient_density\n\n\n\n  \n\n\n\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Calcium\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\")\n             , y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Calcium Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Calcium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nOut of all food sources, Calcium density (the average amount of nutrient for each 1,000 calories in a person’s diet) was highest in schools for 2 out of the 3 income levels (Low and Middle Income). For example, for each 1,000 calories consumed, Low and Middle income individuals consumed an average of 826.8mg and 904.09 mg of calcium at school daily. However, Calcium density for High income individuals was highest when eating at other away-from-home places. For each 1,000 calories consumed, High Income Individuals consumed an average of 760.69 mg of Calcium at other away-from-home places daily.\ncompare magnitudes between highest and 2nd highest calcium density level (show that low income has largest gap in calcium densities )\nLow Income individuals consume a daily average of 597.13mg of calcium per 1,000 calories at Home, which is higher than amounts consumed at home for Middle Income (514.79mg) and High Income (571.69mg) individuals. For Food away from home excluding school (Fast Food, Restaurant, other), Low Income individuals have the lowest Calcium density in each of those categories compared to Middle and High Income individuals. For example, Low Income individuals consumed a daily average of 351.11 mg of calcium per 1,000 calories at other away-from-home places, which is much lower compared to Middle Income (838.37 mg) and High Income (760.69 mg).\nLow Income individuals have the highest Calcium density at Home compared to Middle and High Income individuals. For Food away from home excluding school (Fast Food, Restaurant, other) Low Income individuals have the lowest Calcium density in each of those categories compared to Middle and High Income individuals.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Saturated_fat\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Saturated Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Saturated Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, all age groups had higher daily average intakes of Saturated Fat at away from home food sources (Total) compared to at home food sources.In addition, Saturated Fat density at Fast Food restaurants ranked the highest for each Age group at out of all food sources. Adults consume 2.49 more grams of saturated fat per 1,000 calories at Fast Food restaurants compared to at home, followed by Seniors (2.37g), and Children (2.24g).\nFor Adults and Seniors, Saturated Fat density at Home ranked the lowest compared to all food sources away from home. For Children However, Saturated Fat density at school ranked the lowest compared to at home food sources and all other food sources away from Home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_fats\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Discretionary Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source:2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor all age groups, Discretionary Fat density was lower at Home compared to away from Home. However, the average amount of daily Discretionary Fat per 1,00 calories in ones diet for away from home food sources excluding school (Fast Food, Restaurant, other) rank differently for each age group. For example, other away-from-home places, Fast food, and Restaurants rank 1st, 2nd and 3rd in Discretionary Fat density for Adults and Seniors, respectfully. On the contrary, Fast food ranked 1st other away-from-home places ranked 2nd and Restaurants ranked 3rd in Discretionary Fat density for Children (density values for Fast food, Restaurants and other away from home food sources are 18.4 g/1,000kcal, 17.92 g/1,000kcal and 16.23g/kcal, respectfully).\nThe gap between away-from-home places Discretionary Fat density and At home Discretionary Fat density is much higher for Seniors than for Children and Adults. Among Seniors, other away-from-home places Discretionary Fat density is higher than At home Discretionary Fat density by 4.01 g/1,000kcal. Among Children and Adults, other away-from-home places Discretionary Fat density is higher than At home Discretionary Fat density 1.39 g/1,000kcal and by 3.03 g/1,000kcal, respectfully.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_oils\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Oil Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Discretionary Oils (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor all age groups, Discretionary Oils Density was highest when people consumed food at Restaurants compared to at home and other away from home food sources (Fast Food, Other). For example, for each 1,000 calories, Adults consume a daily average of 19.69g of Discretionary oils when eating at Restaurants, which is a higher amount compared to at home (11.79g), at Fast Food establishments (15.05g), and at other away-from-home places (10.5g).\nMore specifically, Children have the largest difference in Discretionary Oils Density between at home and at Fast food restaurants compared to Adults and seniors. For example, the difference in Discretionary Oils Density between at home and at Fast food restaurants for Children is 4.64g per 1000 calories, which is a larger difference than adults (3.28g/1000kcal) and Seniors (1.26/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Added_sugar\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Added Sugar Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Added Sugar (tsp/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, Added sugar density was higher when individuals consumed food at other away-from-home sources compared to at home food sources for all age groups. However, the difference in Added sugar density between other away-from-home and at home food sources is largest for children compared to Adults and Seniors. Among Children, the difference in Added sugar density between at home and other away-from-home sources is 5.59tsp per 1000 calories, which is a larger difference than Adults (2.37tsp/1000kcal) and Seniors (1.98tsp/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Dairy\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Dairy Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Dairy (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn this figure, Seniors have the largest difference in Dairy Density between at home and total away from home food sources compared to Adults and Children. For example, for each 1,000 calories, Seniors consume a daily average of 0.25 less cups of Dairy when eating away from home compared to at home. Children are consuming more dairy per 1,000 calories at school compared to all other food sources\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Fruit\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fruit Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Fruit (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, Adults consume less fruit per 1000 calories at home than Children and Seniors. For each 1,000 calories, Adults consumed a daily average of 0.59 cups of Fruit at home, which is 0.16 and 0.32 cups less than the amount consumed by Seniors (0.75 cups) and Children (0.91 cups), respectively. Adults and Seniors Daily average intakes of Fruit per 1,000 calories at home are over 3 times more than away from home. Children are consuming more Fruit per 1,000 calories at school compared to all other food sources.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Vegetables_total\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Vegetable Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Vegetable (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nFor each 1,000 calories, children consume a daily average of 0.47 cups of Vegetables at home, which is lower than Adults (0.68 cups) and Seniors (0.74 cups).\nFor all age groups, Total Vegetable Density was highest when people consumed food at Restaurants compared to at home and other away from home food sources (Fast Food, Other). In addition, Children consumed 0.78 cups of vegetables per 1000 calories, which is 0.36 cups/1000kcal and 0.42 cups/1000kcal less than Adults (1.14 cups/1000kcal) and Seniors (1.23 cups/1000kcal), respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Grains\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Grain Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Grains (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for each 1000 calories, the daily average intake of total grains is smaller when the food source is from Home compared to total away from home for all age groups. Adults have the largest difference in the daily average intake of total grains between at home and total away from home sources and lowest at home grain intake . For example, for every 1000 calories, Adults consume 2.79oz of grains at home, which is 0.37 oz/1000kcal less than the amount they consume away from home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Protein_foods\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Protein Foods Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Protien Foods (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average intake of proteins foods was highest at Restaurants compared to at home and and other away from home food sources (Fast Food, Other) for all age groups. For example, Adults consumes a daily average of 4.2 ounces of Protein Foods per 1,000 calories at restaurants, 1.48oz higher than the amount consumed at Home (2.72 oz/1000kcal).\nChildren are consuming about the same amount of Protein foods per 1,000 calories at home (2.12 oz/1000kcal) and at away from home (2.11 oz/1000kcal), whereas Adults and Seniors are consuming more Protein foods away from home compared to at home. In addition, for every 1,000 calories, Seniors and Adults are consuming 0.38 and 0.28 more ounces of protein foods away from home than at home, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Protein_foods\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Protein Foods Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Protien Foods (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nTypically, Low Income and Middle income individuals are consuming more Protein foods per 1,000 calories at Fast Food establishments and restaurants compared to at home. For example, for every 1,000 calories, Low income individuals consume a daily average of 4.1 and 2.7 ounces of Protein Foods at restaurants and Fast food establishments, respectively, which is more than what they consume at home (2.45oz). However, High Income individuals consume more protein foods per 1,000 calories at home (2.73 oz) compared to Fast food establishments (2.26 0z), but not Restaurants (4.25 oz).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Grains\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Grain Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Grains (oz/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average intake of Grains is smaller when the food source is from Home compared to total away from home for all levels of income. Specifically, High income individuals have the largest difference in Grain density, as they consume 0.45 less ounces of Grains per 1,000 calories at home than they do away from home ( Grain densities At home and away from home are 2.83 oz/1000kcal and 3.28 oz/1000kcal, respectively).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Vegetables_total\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Vegetable Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Vegetable (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of Vegetables was higher when the food source is from Restaurants compared to at home and other away from home food sources (Fast Food, Other). In addition the daily average consumption of Vegetables is almost the same at home compared to the total away from home for all income levels. The difference in vegetable densities between at home and total away from home are all lower than 0.15 cups/1000kcal for low, middle and high incomes.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Fruit\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fruit Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Fruit (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, low income individuals have the highest daily average consumption of Fruit away from home, which is 0.16 cups higher than middle income (0.26cup/1000kcal) and double the amount consumed by high income individuals (0.19 cup/1000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Dairy\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Dairy Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Dairy (cup/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, there is little difference between the daily average amount of dairy intake per 1,000 calories at home and away from home food sources for all income levels. More specifically, there is a less than 0.1 cup/1000kcal difference between at home and away from home food sources for all income levels.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Added_sugar\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Added Sugar Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Added Sugar (tsp/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of added sugars was higher when the food source is from Home compared to total away from home for all income levels. However, other away-from-home sources (street vendors, vending machines, etc.) had the highest added sugar density compared to Fast Food establishments and Restaurants for all income levels. For example, the daily average consumption of added sugars for low income individuals at other away-from-home sources was 12.46 teaspoons per 1,000 calories, which is higher than the amount consumed at Fast Food establishments (7.46 tsp/1,000kcal) and Restaurants (4.84 tsp/1,000kcal).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_oils\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Oil Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Discretionary Oil (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, the daily average consumption of Discretionary oil was higher when the food sources is away from home compared to at home for all income levels. In addition, restaurants provide the highest intake of daily Discretionary oils, followed by Fast food establishments and then other away-from-home food sources. For example, High income individuals consumed a daily average of 20 grams per 1,000 calories when eating at Restaurants, which is higher than what they consume at Fast food establishments (14.29 grams) and at other away-from-home food sources (10.9 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\nfood_density %>% \n  filter(year== \"2017-2018\", food_type == \"Discretionary_fats\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = food_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = food_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Discretionary Fat Density by Food Source for Low, Middle, and High Levels of Income, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Discretionary Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, other away from home food sources had the highest daily average intake of Discretionary fat compared to all other away from home food sources (Fast Food ,Restaurant) for 2 of the 3 income levels. However, for Low income individuals, Fast Food establishments had the highest daily average intake of Discretionary fat per 1,000 calories (17.98 grams) compared to all other away from home food sources (densities for other away from home food sources and Restaurants are 16.76g and 14.91g, respectively)\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Cholesterol\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Cholesterol Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Cholesterol (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, High income individuals have a higher Cholesterol density when consuming food away from home compared to middle and low income individuals. In addition, for ever 1,000 calories, High income indivuals consumed a daily average of 153.21 mg of Cholesterol away from home, which is 25.47mg and 21.99mg higher than Middle and Low income individuals, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Fiber\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fiber Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Fiber (g/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, at home food sources had the highest daily average intake of Fiber compared to away from home food sources for all income levels. However, High income individuals had the highest difference in daily fiber consumption per 1,000 calories between at home and away from home food sources, followed by Middle income and low income individuals. For example, the difference in the daily average fiber intake per 1,000 calories for High income individuals between at home and away at home food sources is 2.1 grams, which is higher than the difference for Middle (1.46 grams) and Low income Individuals (0.93 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Iron\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Iron Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Iron (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Iron compared to away from home food sources for all income levels.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Saturated_fat\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Saturated Fat Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Saturated Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, the daily average consumption of Saturated Fat per 1,000 calories was highest at Fast Food establishments compared to at home, restaurants, and other away-from-home food sources for all levels of income. In addition, Low income individuals had the largest difference in daily Saturated Fat consumption per 1,000 calories between at home and Fast food sources, followed by Middle income and High income individuals. For example, the daily average Saturated fat intake per 1,000 calories for Low income individuals at Fast Food establishments is 2.86 grams higher than at home, which is larger than the difference for Middle (2.34 grams) and High income Individuals (1.92 grams). However, High income individuals had the largest difference in daily Saturated Fat consumption per 1,000 calories between at home and other away-from-home food sources, consuming 1.57 g/1,0000kcal more at Fast food establishments compared to at home.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Total_fat\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Fat Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Total Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Total Fat compared to at home food sources for all income levels. In addition, the daily average Total fat intake per 1,000 calories for Middle income individuals away from home is 5.57 grams higher than at home, which is larger than the difference for Low (5.41 grams) and High income Individuals (4.97 grams).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Sodium\",\n  Demographics == \"Household income 185–300% poverty line\" | Demographics == \"Household income < 185% poverty line\" | Demographics == \"Household income > 300% poverty line\" ) %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Household income < 185% poverty line\",\"Household income 185–300% poverty line\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Sodium Density by Food Source for Low, Middle and High Income Levels, 2017-2018\",\n    x = \"Income Level\",\n    y = \"Sodium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Sodium compared to at home food sources for all income levels.In addition, the daily average Sodium intake per 1,000 calories for High income individuals away from home is 572.92 mgs higher than at home, which is larger than the difference for Middle (445.61 mg) and Low income Individuals (323.87 grams). Looking specifically at other away from home food sources, High income individuals consumed a daily average of 2386.09 mg per 1,000 calories, which is 387 mg and 685.55 mg more than Middle and low income individuals, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Calcium\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Calcium Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Calcium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, At Home food sources had the highest daily average intake of calcium compared to all away from home food sources (Fast Food ,Restaurant, Other) for 2 of the 3 age groups. However, Adults had the highest daily average intake of calcium when eating at other away from home food sources (811.39 mg/1,000 calories) compared to At home (584.18 mg/1,000 calories), Fast Food establishments (490.21 mg/1,000 calories), and Restaurants (347.94 mg/1,000 calories). Seniors have the lowest daily average intake of calcium per 1,000 calories at home (506.7 mg), which is 77.48 mg and 83.13mg less than Adults and Children, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Cholesterol\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Cholesterol Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Cholesterol (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, Adults and Seniors had higher daily average intakes of Cholesterol at away from home food sources compared to at home food sources. In addition, the daily average Cholesterol intake per 1,000 calories for Seniors eating away from home is 165.5 mg, which is 16.49 mg higher than the the daily average intake for Adults (149.01 mg). However, Children have a lower daily average intake of Cholesterol at away from home food sources compared to at home food sources. Out of all away from home food sources (Fast Food ,Restaurant, Other), Restaurants have the highest daily average intake of Cholesterol for all age groups.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Fiber\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Fiber Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Fiber (g/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Fiber compared to away from home food sources for all age groups. In addition, the daily average Fiber intake per 1,000 calories for Seniors eating away from home is 2.58 grams less than what they intake at home, which is a larger difference than Adults (1.55g, 8.21 at home, 6.66 away from home) and Children (0.82g, 8.17g at home, 7.37 away from home).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Iron\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Iron Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Iron (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn general, for every 1,000 calories, at home food sources had the highest daily average intake of Iron compared to away from home food sources for all age groups. In addition, the daily average Iron intake per 1,000 calories for Children eating away from home is 2.08 mg less than what they intake at home, which is a larger difference than Adults (1.26 mg, 7.09 mg at home, 5.83 mg away from home) and Seniors (1.39 mg, 7.78 mg at home, 6.39 mg away from home).\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Total_fat\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Total Fat Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Total Fat (g/1,000kcal)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Total compared to at home food sources for all age groups. In addition, the daily average Fat intake per 1,000 calories for Adults eating away from home was 6.18 g more than what they comsume at home, which is a larger difference than Children (2.95 g, 36.15 g at home, 39.4 g away from home) and Seniors (5.37 g, 44.21 at home, 38.84g away from home). However, the daily average Fat intake per 1,000 calories for Children eating at Restaurants (44.04 g) and Fast food (43.02 g) establishment is 7.89 g and 6.87 g more than what they consume at home, respectively.\n\ntheme_set(\n  theme_classic() +\n    theme(legend.position = \"top\")\n  )\n\n\nfood_nutrient_density %>% \n  filter(year== \"2017-2018\", nutrient_type == \"Sodium\",\n  Demographics == \"Children age 2–19\" | Demographics == \"Adults age 20–64\" | Demographics == \"Seniors age 65 and above\") %>% \n  ggplot(aes(x = fct_relevel(Demographics, \"Children age 2–19\"), y = values)) + \n  geom_col(mapping = aes(fill = nutrient_source),colour=\"black\", position = \"dodge\") + \n  geom_text(\n  aes(label = values, group = nutrient_source), \n  position = position_dodge(0.9),\n  vjust = -0.3, size = 2.0) +\n  labs(\n     title = \"Sodium Density by Food Source for Children, Adults and Seniors, 2017-2018\",\n    x = \"Age\",\n    y = \"Sodium (mg/1,000 calories)\",\n    fill = \"Food Source\") +\n    scale_fill_discrete(labels = c(\"At Home\", \"Fast Food\", \"Other\",\"Restaurant\",\"School\",\"Total\", \"Total Away from Home\")) +\n  labs(caption = \"Source: 2017–18 National Health and Nutrition Examination Survey (NHANES), first-day averages.\nNote: School is omitted from Adults and Seniors as School foods eaten by those age 20 and above are classified as other food away from home.\") +\n  theme(plot.caption = element_text(hjust=0))\n\n\n\n\nIn General, for every 1,000 calories, away from home food sources (Total) had the highest daily average intake of Sodium compared to at home food sources for all age groups. However, Children and Seniors are consuming more Sodium at Restaurants compared to at home, Whereas Adults are consuming more sodium at Other away-from-home food sources. For example, the daily average Sodium intakes per 1,000 calories for Children and Seniors eating at restaurants were 617.9 mg and 118.14 more than what they consume at home respectively, Whereas Adults eating at Other away-from-home places consume 936.16 more grams of Sodium than they do at home."
  },
  {
    "objectID": "templates/AboutTemplate.html",
    "href": "templates/AboutTemplate.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "templates/AboutTemplate.html#educationwork-background",
    "href": "templates/AboutTemplate.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "templates/AboutTemplate.html#r-experience",
    "href": "templates/AboutTemplate.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "templates/AboutTemplate.html#research-interests",
    "href": "templates/AboutTemplate.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "templates/AboutTemplate.html#hometown",
    "href": "templates/AboutTemplate.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "templates/AboutTemplate.html#hobbies",
    "href": "templates/AboutTemplate.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "templates/AboutTemplate.html#fun-fact",
    "href": "templates/AboutTemplate.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "templates/PostTemplate.html",
    "href": "templates/PostTemplate.html",
    "title": "Blog Post Template",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "templates/PostTemplate.html#instructions",
    "href": "templates/PostTemplate.html#instructions",
    "title": "Blog Post Template",
    "section": "Instructions",
    "text": "Instructions\nThis document provides yaml header inforamtion you will need to replicate each week to submit your homework or other blog posts. Please observe the following conventions:\n\nSave your own copy of this template as a blog post in the posts folder.\nEdit the yaml header to change your author name\ninclude a description that is reader friendly\nupdate the category list to indicate what category is the blog post, the data used, the main packages or techniques, your name, or any thing else to make your document easy to find\nedit as a normal qmd/rmd file\n\n\n\nCode\nx <- c(2,3,4,5)\nmean(x)\n\n\n[1] 3.5"
  },
  {
    "objectID": "templates/PostTemplate.html#rendering-your-post",
    "href": "templates/PostTemplate.html#rendering-your-post",
    "title": "Blog Post Template",
    "section": "Rendering your post",
    "text": "Rendering your post\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure that you have moved your *.qmd file into the posts folder BEFORE you render it, so that all files are stored in the correct location.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnly render a single file - don’t try to render the whole website!\n\n\n\n\n\n\n\n\nPilot Student Blogs\n\n\n\nWe are piloting a workflow including individual student websites with direted and limited pull requests back to course blogs. Please let us know if you would like to participate."
  },
  {
    "objectID": "templates/PostTemplate.html#reading-in-data-files",
    "href": "templates/PostTemplate.html#reading-in-data-files",
    "title": "Blog Post Template",
    "section": "Reading in data files",
    "text": "Reading in data files\nThe easiest data source to use - at least initially - is to choose something easily accessible, either from our _data folder provided, or from an online source that is publicly available.\n\n\n\n\n\n\nUsing Other Data\n\n\n\nIf you would like to use a source that you have access to and it is small enough and you don’t mind making it public, you can copy it into the _data file and include in your commit and pull request.\n\n\n\n\n\n\n\n\nUsing Private Data\n\n\n\nIf you would like to use a proprietary source of data, that should be possible using the same process outlined above. There may initially be a few issues. We hope to have this feature working smoothly soon!"
  }
]
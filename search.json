[
  {
    "objectID": "about/Nayan.html",
    "href": "about/Nayan.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "Education\nUniversity of Massachusetts Amherst, College of Social and Behavioral Sciences, Amherst, MA. Exp. Graduation:: Jan 2024 Masters Concentration: Data Analytics and Computational Social Science Current GPA: 3.70 Relevant Courses: Text as Data, Regression Models, Digital Behavior of Data, Machine Learning for Social Science\nUniversity of Rhode Island, College of Arts and Sciences, Kingston, RI. Graduated:: May 2022 Bachelor of Science Concentration: Data Science Final GPA: 3.52, Dean’s List: 2018, 2019, 2020, 2021, 2022 Relevant Courses: Machine Learning, Multivariate Statistical Learning, Big Data Analysis, Database Management\n\n\nSkills\nSoft and Technical Skills: Active Listener, Adaptability, Decision Making, Critical Thinking, Leadership, Open-Mindedness, Teamworker, Data Visualization, Data Wrangling\nProgramming Languages: Python, R, HTML and SQL\nLibraries: Ggplot2, Tidy verse, Summarytools, Stats, Tidytext, Quanteda, Rselenium, Lubridate, Devtools, SciPy, Matplotlib, Seaborn, Scikit-learn, Numpy, Pandas\n\n\nProjects\nFebruary-May 2023 Fixing Social Media Design Brief Team Member - Developed a design brief for an app called “Social Media Royale” that helps reduce people’s screen time with 2 student colleagues - Conducted background research and interviews over Zoom to understand the consequences of high amounts of screen time - Brainstormed many design ideas that were is used in the final pitch with my team members - Gained substantial knowledge about the process of product design\nLink: https://github.com/Njani01886/Website/blob/main/projects/Slide%20for%20SMR.pdf\nFebruary-May 2023 Research Design Final Project Team Member - Conducted a survey experiment research study with the goal of understanding if exposure to misinformation increases belief in conspiracy theories with two student colleagues - Defined constructs, operational definitions, ways of measurement for constructs, and treatment groups for the research study - Gained experience in using Qualtrics for distributing the survey - Analyzed results from the survey using stacked bar charts, One-Way Anova, and Linear Regression using R\nLink: https://github.com/Njani01886/Website/blob/main/projects/602_Poster.pptx.pdf\nFebruary-May 2023 Machine Learning Final Project Author\n\nDeveloped and tested four different machine learning models using different regression methods to explore which one performs the best at predicting NBA players’ salaries\nConducted exploratory data analysis, data wrangling, data preprocessing and hyperparameter selection\nCompared and evaluated models in terms of overfitting vs underfitting, bias vs variance tradeoff, flexibility vs interpretability\nUsed Python and R\n\nLink: https://github.com/Njani01886/Website/blob/main/projects/ML%20final%20poster.pdf\nNovember-December 2022 Regression Models Final Project, Amherst, MA\nTeam Member - Developed a Quasi-Poisson regression model using R over a 2 month period with 3 student colleagues - Effectively communicated with my team members to establish roles and responsibilities - Utilized my decision making and critical thinking skills during each team meeting to help develop our methodology - Presented our work in a professional manner in front of my peers\nLink: https://github.com/Njani01886/Website/blob/main/projects/Soccer_Regression.pdf\nNovember-December 2022 Text as Data Final Project, Amherst, MA Author\nFor this project I analyzed controversies of the Qatar World Cup by conducting Dictionary and Sentiment Analysis, LDA Topic Modeling and Semantic Network and Pairwise Correlation Analysis on Youtube comments about this subject. My main goal was to find what topics are most important to the people and if the comments point of view were acceptable or stigmatized.\nHere I will provide two links: One for my code and one for my final poster. Note: The analyses I made in my code post were not my final conclusions. My final conclusions were made on my final poster.\nCode: (https://dacss.github.io/Text_as_Data_Fall_2022/posts/BlogPost6_NayanJani.html) Final Poster: https://github.com/Njani01886/Website/blob/main/projects/World_Cup_Text_Analysis.png\nSpring 2022 Machine Learning Project at URI, Kingston, RI Research Assistant - Designed a program in Python that estimated heterogeneous treatment effects using datasets that involve AIDS and breast cancer treatments as a part of my senior dissertation - Implemented different meta learners in my program that could estimate the conditional average treatment effect to identify how to personalize treatment regimes - Gained experience and knowledge in Causal Inference - Discussed methodology weekly with research leader\n\n\nExperience\nSummer 2021 Bora, La Jolla, CA Business Development & Land Acquisition - Compiled data about potential locations using Google Maps - Analyzed aggregated data to determine which locations are the busiest based on area busyness - Company’s goal is to bring to market an app based, self service beach chair rental service stationed at the most popular locations across the country - Part of outreach campaign to early stage VC and angel investors\nSpring 2018 Westford Academy Spring Recreational Basketball, Westford, MA Youth Basketball Coach/League Co-Director - Organized and operated a Spring League consisting of 80 players with 3 student colleagues as part of my senior year internship - Successfully coordinated a three month league for the Westford Youth Association. All surplus proceeds were donated back to the Westford Academy Girls Basketball Program - Established online sign-up and jersey purchases - Coordinated facilities and scheduling of refereeing staff\n\n\nHometown\nWestford, MA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nayan Jani",
    "section": "",
    "text": "I am currently a Data Analytics and Computational Social Science Masters student at UMass Amherst with a passion for solving complex problems that improve business decisions and lives of other people. I have always been intrigued by the power of data, and pursuing a Master’s degree in Data Analytics has allowed me to explore this field in depth.\nThroughout my academic and professional journey, I’ve developed strong skills in data analysis, statistics, and machine learning, and I’m always eager to learn new techniques and tools to better serve my clients. I’ve worked on projects ranging from predictive modeling for housing markets to social media sentiment analysis, and I enjoy using my skills to find insights and patterns that can inform decision-making and drive positive change.\nAdditionally, I have completed several projects that required me to collect, clean, and analyze large sets of data, which provided me with hands-on experience with data cleaning and scraping, feature engineering, model selection, and evaluation using tools such as SQL, R, and Python. My previous internship at Bora, a startup company, enabled me to work with a variety of stakeholders, providing me with experience in working collaboratively, which I believe is essential for success.\nAs a person, I’m motivated by a desire to help others. I believe that data can be a powerful force for improving the quality of life of any individual, and I’m committed to using my skills to help businesses and individuals achieve their goals. Whether I’m working on a complex analysis or brainstorming ideas with colleagues, I always strive to be supportive, empathetic, and collaborative, and I enjoy working with people from diverse backgrounds and perspectives. I’m excited to be part of the next generation of data professionals who will make a positive impact in the world.\nIf you’re interested in connecting with me or learning more about my work, please don’t hesitate to reach out."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Nayan Jani",
    "section": "Research interests",
    "text": "Research interests\nThe topic I explored for my Senior recitation is the use of Machine Learning in estimating the heterogeneous treatment effect using datasets that involve AIDS and breast cancer treatments. More specifically, I looked into certain meta learners that can estimate the conditional average treatment effect so that we can personalize treatment regimes. Here at Umass I hope to learn more about how to apply data science to medical treatments for people with illnesses."
  },
  {
    "objectID": "about.html#hometown",
    "href": "about.html#hometown",
    "title": "Nayan Jani",
    "section": "Hometown",
    "text": "Hometown\nWestford, MA"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "Nayan Jani",
    "section": "Hobbies",
    "text": "Hobbies\n\nVideo Games\nFantasy Football\nWatching Sports\nPlaying Cards and Board Games"
  },
  {
    "objectID": "about.html#fun-fact",
    "href": "about.html#fun-fact",
    "title": "Nayan Jani",
    "section": "Fun fact",
    "text": "Fun fact\nI have over 50 cousins that live in the UK!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601 August 2022",
    "section": "",
    "text": "NBA Salary Prediction\n\n\n\n\n\n\n\nNBA\n\n\nMachine Learning\n\n\n\n\nMachine Learning for the Social Sciences\n\n\n\n\n\n\nJun 12, 2023\n\n\nNayan Jani\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/NBA_Salary.html",
    "href": "posts/NBA_Salary.html",
    "title": "NBA Salary Prediction",
    "section": "",
    "text": "Code\nlibrary(alr4)\nlibrary(tinytex)\nlibrary(summarytools)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(aod)\nlibrary(DescTools)\nlibrary(MASS)\nlibrary(leaps)\nlibrary(GGally)\nlibrary(hrbrthemes)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "posts/NBA_Salary.html#introduction",
    "href": "posts/NBA_Salary.html#introduction",
    "title": "NBA Salary Prediction",
    "section": "Introduction",
    "text": "Introduction\nBeing a General Manager in the NBA comes with a ton of decisions. One of the most important decisions a GM can make is how much they pay the players on their team. It is so important to pay the players the right amount in order to build the strongest roster. Overpaying a player will hurt a teams cap space, meaning that the team will not be able to sign good players because they do not have enough money to afford them. My motivation for this project its to see if Machine Learning techniques can correctly predict a players salary. The idea is if I am able to create a model that performs well enough, then it could be used as a tool to determine a players salary for their next contract. Here I will perform different regression methods to predict players salary and then used the best method for prediction."
  },
  {
    "objectID": "posts/NBA_Salary.html#the-data",
    "href": "posts/NBA_Salary.html#the-data",
    "title": "NBA Salary Prediction",
    "section": "The Data",
    "text": "The Data\nThe dataset I am using comes from Kaggle. The dataset contains information about player names, time span of the contract, avg salary per year and all stats that player accumulated during NBA season before signing their next contract. The scope of the data is as follows: - There are only contracts signed since 2010/2011 season to 2019/2020 season. - Only includes players that are active in 2020/2021 season. - Doesn’t include rookie or retained contracts. - Doesn’t include contracts for player that haven’t played year before the signing the contract.\nThis is a good scope because I want to use modern players contracts for future predictions. The limitation of only including players that are active in 20/21 means that these players were able to earn multiple contracts of the 10 year span, which validates them as players who are worth to continuing paying. Having this removes players who had massive contracts early in their career and then faded out quickly after their primes.\n\n\nCode\ndf<- read_csv(\"_data/nba_contracts_history.csv\")\n\n\nRows: 199 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (27): CONTRACT_START, CONTRACT_END, AVG_SALARY, AGE, GP, W, L, MIN, PTS,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ndf\n\n\n# A tibble: 199 × 28\n   NAME  CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n   <chr>   <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Wesl…    2019    2020  2.56e6    32    69    27    42  2091   840   279   698\n 2 Broo…    2015    2017  2.12e7    27    72    34    38  2100  1236   506   987\n 3 DeAn…    2011    2014  1.08e7    22    80    31    49  2047   566   234   341\n 4 Mark…    2015    2018  8.14e6    25    82    39    43  2581  1258   512  1100\n 5 Dwig…    2018    2019  1.34e7    32    81    35    46  2463  1347   506   911\n 6 Aust…    2015    2016  7.06e6    22    76    46    30  1563   530   203   496\n 7 Wayn…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n 8 JaMy…    2019    2020  4.77e6    29    65    31    34  1371   611   230   476\n 9 Kyle…    2015    2018  4.05e6    25    51    15    36   824   294   119   242\n10 Trev…    2014    2017  8   e6    28    77    41    36  2723  1107   389   853\n# … with 189 more rows, 16 more variables: `FG%` <dbl>, `3PM` <dbl>,\n#   `3PA` <dbl>, `3P%` <dbl>, FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>,\n#   DREB <dbl>, REB <dbl>, AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>,\n#   PF <dbl>, `+/-` <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\n\n\nCode\ndf<- df %>% mutate(c_duration = CONTRACT_END -CONTRACT_START)\n\n\nAfter loading in the data I can see it has 199 instances with 29 features. Some features will be removed such as player name, CONTRACT_START and CONTRACT_end. After EDA I will remove any features that interfere with my regression analysis. The only variable mutation I did was c_duration, which is how long a players contract lasted."
  },
  {
    "objectID": "posts/NBA_Salary.html#eda",
    "href": "posts/NBA_Salary.html#eda",
    "title": "NBA Salary Prediction",
    "section": "EDA",
    "text": "EDA\n\n\nCode\nggplot(data=df, aes(x=AGE, y=AVG_SALARY)) +\n  geom_bar(stat=\"identity\") +\n  labs(title = \"Average Salary by Age\", x = \"Age\" ,y= \"Average Salary\")\n\n\n\n\n\nThe first relationship I wanted to visualize was Average salary and Age. Age is very important when thinking about how much to pay someone because you want to give them a contract that shows how good they can perform for that length of contract. I can see that average salary peaks at age 23 and again rises at 27 and then 30. This make sense because young players who are really good will receive a massive 2nd contract (contract after rookie deal). The peaks at 27 and 30 could be from players signing their 3rd or 4th contracts but make less money due to their age.\n\n\nCode\nprint(dfSummary(df, varnumbers = FALSE,\n                        plain.ascii  = FALSE, \n                        style        = \"grid\", \n                        graph.magnif = 0.70, \n                        valid.col    = FALSE),\n      method = 'render',\n      table.classes = 'table-condensed')\n\n\n\n\nData Frame Summary\ndf\nDimensions: 199 x 29\n  Duplicates: 1\n\n\n  \n    \n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Missing\n    \n  \n  \n    \n      NAME\n[character]\n      1. Kevin Durant2. Austin Rivers3. Avery Bradley4. Danilo Gallinari5. Dwight Howard6. E'Twaun Moore7. Paul Millsap8. Quinn Cook9. Robin Lopez10. Wayne Ellington[ 128 others ]\n      4(2.0%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)3(1.5%)168(84.4%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_START\n[numeric]\n      Mean (sd) : 2015.2 (2.1)min ≤ med ≤ max:2011 ≤ 2015 ≤ 2019IQR (CV) : 3 (0)\n      2011:9(4.5%)2012:16(8.0%)2013:16(8.0%)2014:21(10.6%)2015:48(24.1%)2016:35(17.6%)2017:23(11.6%)2018:20(10.1%)2019:11(5.5%)\n      \n      0\n(0.0%)\n    \n    \n      CONTRACT_END\n[numeric]\n      Mean (sd) : 2017.5 (1.7)min ≤ med ≤ max:2013 ≤ 2018 ≤ 2020IQR (CV) : 3 (0)\n      2013:2(1.0%)2014:11(5.5%)2015:18(9.0%)2016:23(11.6%)2017:29(14.6%)2018:46(23.1%)2019:55(27.6%)2020:15(7.5%)\n      \n      0\n(0.0%)\n    \n    \n      AVG_SALARY\n[numeric]\n      Mean (sd) : 11073609 (7897820)min ≤ med ≤ max:823244 ≤ 9500000 ≤ 33599500IQR (CV) : 11621898 (0.7)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AGE\n[numeric]\n      Mean (sd) : 25.9 (2.8)min ≤ med ≤ max:20 ≤ 25 ≤ 36IQR (CV) : 4 (0.1)\n      16 distinct values\n      \n      0\n(0.0%)\n    \n    \n      GP\n[numeric]\n      Mean (sd) : 64.2 (19.6)min ≤ med ≤ max:1 ≤ 72 ≤ 82IQR (CV) : 19 (0.3)\n      59 distinct values\n      \n      0\n(0.0%)\n    \n    \n      W\n[numeric]\n      Mean (sd) : 34.2 (14.5)min ≤ med ≤ max:0 ≤ 35 ≤ 64IQR (CV) : 20 (0.4)\n      56 distinct values\n      \n      0\n(0.0%)\n    \n    \n      L\n[numeric]\n      Mean (sd) : 30 (13)min ≤ med ≤ max:0 ≤ 31 ≤ 62IQR (CV) : 18.5 (0.4)\n      53 distinct values\n      \n      0\n(0.0%)\n    \n    \n      MIN\n[numeric]\n      Mean (sd) : 1747 (782.4)min ≤ med ≤ max:2 ≤ 1867 ≤ 3125IQR (CV) : 1125.5 (0.4)\n      193 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PTS\n[numeric]\n      Mean (sd) : 813.4 (499.9)min ≤ med ≤ max:0 ≤ 734 ≤ 2376IQR (CV) : 710.5 (0.6)\n      186 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGM\n[numeric]\n      Mean (sd) : 300.4 (178.6)min ≤ med ≤ max:0 ≤ 277 ≤ 743IQR (CV) : 261 (0.6)\n      172 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FGA\n[numeric]\n      Mean (sd) : 638.8 (374.6)min ≤ med ≤ max:1 ≤ 572 ≤ 1643IQR (CV) : 553.5 (0.6)\n      189 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FG%\n[numeric]\n      Mean (sd) : 46.7 (8.1)min ≤ med ≤ max:0 ≤ 45.5 ≤ 100IQR (CV) : 7 (0.2)\n      131 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PM\n[numeric]\n      Mean (sd) : 63.6 (58.4)min ≤ med ≤ max:0 ≤ 57 ≤ 272IQR (CV) : 91 (0.9)\n      111 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3PA\n[numeric]\n      Mean (sd) : 173.9 (148.7)min ≤ med ≤ max:0 ≤ 170 ≤ 657IQR (CV) : 241 (0.9)\n      139 distinct values\n      \n      0\n(0.0%)\n    \n    \n      3P%\n[numeric]\n      Mean (sd) : 29.7 (13.2)min ≤ med ≤ max:0 ≤ 34.8 ≤ 50IQR (CV) : 11.9 (0.4)\n      110 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTM\n[numeric]\n      Mean (sd) : 149 (128.4)min ≤ med ≤ max:0 ≤ 107 ≤ 720IQR (CV) : 143 (0.9)\n      143 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FTA\n[numeric]\n      Mean (sd) : 195.5 (162.2)min ≤ med ≤ max:0 ≤ 145 ≤ 837IQR (CV) : 192.5 (0.8)\n      162 distinct values\n      \n      0\n(0.0%)\n    \n    \n      FT%\n[numeric]\n      Mean (sd) : 74 (15)min ≤ med ≤ max:0 ≤ 76.8 ≤ 100IQR (CV) : 11.4 (0.2)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      OREB\n[numeric]\n      Mean (sd) : 79.3 (72.7)min ≤ med ≤ max:0 ≤ 51 ≤ 397IQR (CV) : 85.5 (0.9)\n      117 distinct values\n      \n      0\n(0.0%)\n    \n    \n      DREB\n[numeric]\n      Mean (sd) : 249 (164)min ≤ med ≤ max:1 ≤ 209 ≤ 829IQR (CV) : 235 (0.7)\n      167 distinct values\n      \n      0\n(0.0%)\n    \n    \n      REB\n[numeric]\n      Mean (sd) : 328.3 (226.1)min ≤ med ≤ max:1 ≤ 286 ≤ 1226IQR (CV) : 305 (0.7)\n      169 distinct values\n      \n      0\n(0.0%)\n    \n    \n      AST\n[numeric]\n      Mean (sd) : 171.7 (163.9)min ≤ med ≤ max:0 ≤ 110 ≤ 839IQR (CV) : 137 (1)\n      147 distinct values\n      \n      0\n(0.0%)\n    \n    \n      TOV\n[numeric]\n      Mean (sd) : 103.4 (70.9)min ≤ med ≤ max:0 ≤ 85 ≤ 374IQR (CV) : 83 (0.7)\n      138 distinct values\n      \n      0\n(0.0%)\n    \n    \n      STL\n[numeric]\n      Mean (sd) : 58.2 (37.3)min ≤ med ≤ max:0 ≤ 48 ≤ 169IQR (CV) : 43 (0.6)\n      100 distinct values\n      \n      0\n(0.0%)\n    \n    \n      BLK\n[numeric]\n      Mean (sd) : 39.6 (43.3)min ≤ med ≤ max:0 ≤ 23 ≤ 269IQR (CV) : 40 (1.1)\n      83 distinct values\n      \n      0\n(0.0%)\n    \n    \n      PF\n[numeric]\n      Mean (sd) : 137.7 (64.3)min ≤ med ≤ max:1 ≤ 137 ≤ 291IQR (CV) : 84 (0.5)\n      135 distinct values\n      \n      0\n(0.0%)\n    \n    \n      +/-\n[numeric]\n      Mean (sd) : 62.9 (227.7)min ≤ med ≤ max:-628 ≤ 44 ≤ 839IQR (CV) : 257.5 (3.6)\n      171 distinct values\n      \n      0\n(0.0%)\n    \n    \n      c_duration\n[numeric]\n      Mean (sd) : 2.3 (1.1)min ≤ med ≤ max:1 ≤ 2 ≤ 4IQR (CV) : 2 (0.5)\n      1:69(34.7%)2:36(18.1%)3:66(33.2%)4:28(14.1%)\n      \n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.0)2023-06-12\n\n\n\nHere I created summary statistics of the dataset using SummaryTools. The two stats that stood out to me was the mean games played and mean minutes played. The GP and MIN mean values are 64.2 and 1747, respectively. This implies that in order to be considered for another contract, players must play most of the season and play about 21 minutes per game. I would say that if you looked at all the means and medians of each feature, then those values represent a player who will get another NBA contract.\n\n\nCode\nggplot(data = df, mapping = aes(x = MIN, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Minutes Played\", x = \"Mins\" ,y= \"Average Salary\")\n\n\n\n\n\nNext I wanted to check the relationship between Minutes played and Average Salary. I can see that their relationship is non-linear but positive. I see an increase of salary once a player is playing roughly 1750 minutes but drops off around 2500 minutes.The positive relationship could suggest that minutes played can be a good predictor.\n\n\nCode\nggplot(data = df, mapping = aes(x = `+/-`, y = AVG_SALARY)) + \n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Average Salary by Plus/Minus\", x = \"+/-\" ,y= \"Average Salary\")\n\n\n\n\n\nHere I wanted to see the relationship between +/- and Average salary. +/- is a sports statistic used to measure a player’s impact on the game, represented by the difference between their team’s total scoring versus their opponent’s when the player is in the game. I can see the relationship between +/- and Average Salary is non linear. I can see that there are players with awful +/- that are getting payed more than players with high +/-. I also see a lot of data grouped around 0 with low salaries, meaning these players probably did not play much.\n\n\nCode\ndf %>% filter(`+/-` < -300)\n\n\n# A tibble: 7 × 29\n  NAME   CONTR…¹ CONTR…² AVG_S…³   AGE    GP     W     L   MIN   PTS   FGM   FGA\n  <chr>    <dbl>   <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Wayne…    2016    2017  6.14e6    28    76    18    58  1615   586   218   561\n2 JaVal…    2012    2015  1.13e7    24    61    22    39  1535   691   307   552\n3 Gordo…    2014    2017  1.90e7    24    77    23    54  2800  1248   426  1032\n4 Jorda…    2016    2019  1.25e7    24    79    17    62  2552  1225   475  1098\n5 Derri…    2014    2017  1.2 e7    22    73    25    48  2201   970   390   747\n6 JaKar…    2015    2016  1.10e6    22    74    17    57  1131   386   146   346\n7 Nikol…    2015    2018  1.2 e7    24    74    21    53  2529  1428   631  1206\n# … with 17 more variables: `FG%` <dbl>, `3PM` <dbl>, `3PA` <dbl>, `3P%` <dbl>,\n#   FTM <dbl>, FTA <dbl>, `FT%` <dbl>, OREB <dbl>, DREB <dbl>, REB <dbl>,\n#   AST <dbl>, TOV <dbl>, STL <dbl>, BLK <dbl>, PF <dbl>, `+/-` <dbl>,\n#   c_duration <dbl>, and abbreviated variable names ¹​CONTRACT_START,\n#   ²​CONTRACT_END, ³​AVG_SALARY\n\n\nHere I investigate the players with terrible +/-. Some of these players are getting paid well but their teams are so bad that their +/- statistic is negative. This makes me believe that non-linear regression methods would be useful if this feature is used for prediction of salary.\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 2:7),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 8:13),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 22:26),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  ggpairs(columns=c(1, 14:20),\n          upper=list(continuous=wrap('cor',size=5)),\n          lower=list(combo=wrap(\"facethist\",bins=30)),\n          diag=list(continuous=wrap(\"densityDiag\"),alpha=0.5))\n\n\n\n\n\n\n\nCode\nlibrary(ggcorrplot)\n\ndf %>% dplyr::select(-c(NAME,CONTRACT_START,CONTRACT_END)) %>% \n  cor() %>% \n  ggcorrplot(hc.order = TRUE, type = \"lower\",outline.col = \"white\",lab=TRUE, lab_size=1)\n\n\n\n\n\nHere I created 3 ggpairs plots and a correlation matrix to investigate the correlations between my target variable and and features. I am looking for strong and weak correlations. Based on the correlation matrix, I see a mixture or strong and weak correlations between features and my target variable, Average Salary. This makes me believe that non linear regression methods will perform better than linear regression methods on this dataset.\n\n\nCode\ndf %>%  ggplot(aes(x=AVG_SALARY)) + \n  geom_density() +\n  geom_vline(aes(xintercept=mean(AVG_SALARY)),\n            color=\"blue\", linetype=\"dashed\", size=1)\n\n\n\n\n\nCode\ndf %>% dplyr::select(AVG_SALARY) %>% \n  summarise(mean = mean(AVG_SALARY))\n\n\n# A tibble: 1 × 1\n       mean\n      <dbl>\n1 11073609.\n\n\nHere I wanted to look at the distribution of my target variable. I can see the distribution is not normal and skewed right. Seeing this makes me think that more flexible methods will work better when I create my regression models."
  },
  {
    "objectID": "posts/NBA_Salary.html#pre-processing-in-python",
    "href": "posts/NBA_Salary.html#pre-processing-in-python",
    "title": "NBA Salary Prediction",
    "section": "Pre Processing in Python",
    "text": "Pre Processing in Python\n\n\nCode\ndf = pd.read_csv(\"_data/nba_contracts_history.csv\")\n\n\n\n\nCode\n#Creating variable c_duration\ndf[\"c_duration\"] = df[\"CONTRACT_END\"] - df[\"CONTRACT_START\"] \n\n\n\n\nCode\n#Creates set of features I will use\nfeatures = df.drop([\"NAME\",\"CONTRACT_START\", \"CONTRACT_END\",\"AVG_SALARY\",\"W\",\"L\"], axis=1)\n\n\n\n\nCode\n#sub-setting my target variable, Average Salary\ntarget = df['AVG_SALARY'].to_numpy()\n\n\nHere is my data splits, 70% training, 15% validation and 15% Test.\n\n\nCode\nX_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.3, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_test, Y_test, test_size=0.5, random_state= 42)\n\n\nHere I check to see if the data is split up correctly.\n\n\nCode\nprint(X_train.shape, X_valid.shape, X_test.shape)\n\n\n(139, 23) (30, 23) (30, 23)\n\n\nCode\nprint(Y_train.shape, Y_valid.shape, Y_test.shape)\n\n\n(139,) (30,) (30,)\n\n\nHere I use the make_scorer() function so that I can call RMSLE during Grid Search Cross Validation.\n\n\nCode\nRMSLE = make_scorer(mean_squared_log_error, squared=False)"
  },
  {
    "objectID": "posts/NBA_Salary.html#random-forest",
    "href": "posts/NBA_Salary.html#random-forest",
    "title": "NBA Salary Prediction",
    "section": "Random Forest",
    "text": "Random Forest\nFor hyperparameter tuning, I chose to tune n_estimators, max_features, min_samples_split and min_sample_leafs. From Sckit learn’s documentation, it states that n_estimators and max_features are the main parameters to tune, so I will follow the documentations suggestion. I included min_sample_split and min_sample_leafs because I want to control my model for overfitting. By increasing the values of these two parameters, it will help prevent my model from overfitting.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\n\nparam_grid_RF = {\n    'n_estimators': [100, 200, 300, 400],\n     'max_features': [None, 1.0],\n     'min_samples_split': [2, 4, 6 ,8],\n    'min_samples_leaf': [1, 2 ,4, 6, 8]}\n    \n    \n    \ngridRF= GridSearchCV(RF, param_grid_RF,scoring = RMSLE, cv=5)\ngridRF.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [None, 1.0],\n                         'min_samples_leaf': [1, 2, 4, 6, 8],\n                         'min_samples_split': [2, 4, 6, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridRF.best_params_)\n\n\nBest parameters: {'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRF.best_score_)\n\n\nTraining error (RMSLE): 0.5703407520704454\n\n\nChecking Validation Error for Random Forests.\n\n\nCode\n\nbest_model_RF = gridRF.best_estimator_\npredict_y_RF = best_model_RF.predict(X_valid)\n\nRF_rmsle = mean_squared_log_error(Y_valid, predict_y_RF,squared=False)\n\n\nprint(\"Validation error (RMSLE):\",RF_rmsle)\n\n\nValidation error (RMSLE): 0.45497526641066893"
  },
  {
    "objectID": "posts/NBA_Salary.html#scaling-data",
    "href": "posts/NBA_Salary.html#scaling-data",
    "title": "NBA Salary Prediction",
    "section": "Scaling Data",
    "text": "Scaling Data\nScaling data for SVM and Ridge Regression.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "posts/NBA_Salary.html#svr",
    "href": "posts/NBA_Salary.html#svr",
    "title": "NBA Salary Prediction",
    "section": "SVR",
    "text": "SVR\nThe hyperparameters I tuned from SVR are C, gamma and kernel. I needed to tune for Kernel so that my model finds the best hyper plane that fits the datapoints from my target variable. The kernel will deal with the non-linear relationship on between my features and target variable. Tuning for C and gamma will help my model from overfitting.\n\n\nCode\nfrom sklearn.svm import SVR\n\nSVR = SVR()\n\nparam_grid_SVR = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [0.0001, 0.001,0.01, 0.1, 1],\n              'kernel': ['rbf', 'poly']}\ngridSVR= GridSearchCV(SVR, param_grid_SVR, scoring = RMSLE, cv=5)\ngridSVR.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=SVR(),\n             param_grid={'C': [0.1, 1, 10, 100, 1000],\n                         'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n                         'kernel': ['rbf', 'poly']},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\",gridSVR.best_params_)\n\n\nBest parameters: {'C': 0.1, 'gamma': 0.0001, 'kernel': 'poly'}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridSVR.best_score_)\n\n\nTraining error (RMSLE): 1.0135970157480874\n\n\nChecking Validation Error for SVR.\n\n\nCode\nbest_model_SVR = gridSVR.best_estimator_\npredict_y_SVR = best_model_SVR.predict(X_valid_scaled)\n\nSVR_rmsle = mean_squared_log_error(Y_valid, predict_y_SVR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", SVR_rmsle)\n\n\nValidation error (RMSLE): 0.6805718407616901"
  },
  {
    "objectID": "posts/NBA_Salary.html#ridge",
    "href": "posts/NBA_Salary.html#ridge",
    "title": "NBA Salary Prediction",
    "section": "Ridge",
    "text": "Ridge\nFor Ridge Regression, The only hyperparameter I tuned was alpha, which is the penalty term. I chose to search through small, intermediate, and large values of alpha.\n\n\nCode\nridge = Ridge()\nparam_grid_ridge = {'alpha':np.concatenate((np.arange(0.1,2,0.1), np.arange(2, 5, 0.5), np.arange(5, 105, 5)))}\ngridRidge = GridSearchCV(ridge, param_grid_ridge,scoring = RMSLE, cv=5)\ngridRidge.fit(X_train_scaled, Y_train)\n\n\nGridSearchCV(cv=5, estimator=Ridge(),\n             param_grid={'alpha': array([  0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.8,   0.9,\n         1. ,   1.1,   1.2,   1.3,   1.4,   1.5,   1.6,   1.7,   1.8,\n         1.9,   2. ,   2.5,   3. ,   3.5,   4. ,   4.5,   5. ,  10. ,\n        15. ,  20. ,  25. ,  30. ,  35. ,  40. ,  45. ,  50. ,  55. ,\n        60. ,  65. ,  70. ,  75. ,  80. ,  85. ,  90. ,  95. , 100. ])},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridRidge.best_params_)\n\n\nBest parameters: {'alpha': 100.0}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridRidge.best_score_)\n\n\nTraining error (RMSLE): 0.5579904916871834\n\n\nChecking Validation Error for Ridge Regression.\n\n\nCode\nbest_model_ridge = gridRidge.best_estimator_\npredict_y_ridge = best_model_ridge.predict(X_valid_scaled)\n\n\nridge_rmsle = mean_squared_log_error(Y_valid, predict_y_ridge,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", ridge_rmsle)\n\n\nValidation error (RMSLE): 0.5200462148165859"
  },
  {
    "objectID": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "href": "posts/NBA_Salary.html#gradient-boosting-decision-trees-regressor",
    "title": "NBA Salary Prediction",
    "section": "Gradient Boosting Decision Trees Regressor",
    "text": "Gradient Boosting Decision Trees Regressor\nThe hyperparameters I chose to tune for the Gradient Boosting Decision Trees Regressor are n_estimators, learning_rate, and max_depth. GBDTR uses multiple shallow decision trees as weak learners to predict the residuals of the decision trees instead of the target variable. The idea here is use gradient descent to update the residuals after every tree until the model has run through all available trees (n_estimators). Learning rate controls how much the residuals are updated from tree to tree, which can also be described as the “size” of the step in gradient descent. Smaller values of learning rate will allow my model to generalize better on the validation and test data. Max_depth refers to the depth of each tree, which is important because it determines how weak the trees are in the ensemble.\n\n\nCode\nfrom sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\n\nparam_grid_GBR = {\n    'n_estimators': [100, 200, 300, 400],\n     'learning_rate': [ 0.01, 0.1, 0.2,0.3],\n     'max_depth': [3,4,5,6,7,8]}\n    \n    \n    \ngridGBR= GridSearchCV(GBR, param_grid_GBR,scoring = RMSLE, cv=5)\ngridGBR.fit(X_train, Y_train)\n\n\nGridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                         'max_depth': [3, 4, 5, 6, 7, 8],\n                         'n_estimators': [100, 200, 300, 400]},\n             scoring=make_scorer(mean_squared_log_error, squared=False))\n\n\nCode\nprint(\"Best parameters:\", gridGBR.best_params_)\n\n\nBest parameters: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}\n\n\nCode\nprint(\"Training error (RMSLE):\", gridGBR.best_score_)\n\n\nTraining error (RMSLE): 0.7298632180107352\n\n\nChecking Validation Error for Gradient Boosting Decision Trees.\n\n\nCode\nbest_model_GBR = gridGBR.best_estimator_\npredict_y_GBR = best_model_GBR.predict(X_valid)\n\n\nGBR_rmsle = mean_squared_log_error(Y_valid, predict_y_GBR,squared=False)\n\n\nprint(\"Validation error (RMSLE):\", GBR_rmsle)\n\n\nValidation error (RMSLE): 0.5732402442589972"
  },
  {
    "objectID": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "href": "posts/NBA_Salary.html#final-evaluation-using-test-set",
    "title": "NBA Salary Prediction",
    "section": "Final Evaluation using Test Set",
    "text": "Final Evaluation using Test Set\nChecking Test Error for all models. Again I will list the best parameters selected from Grid Search CV.\n\n\nCode\n\nbest_model_RF\n\n\nRandomForestRegressor(max_features=None, min_samples_split=6)\n\n\nCode\npredict_test_RF = best_model_RF.predict(X_test)\n\n\nRF_test_rmsle = mean_squared_log_error(Y_test, predict_test_RF,squared=False)\n\n\nprint(\"Test error (RMSLE):\", RF_test_rmsle)\n\n\nTest error (RMSLE): 0.4986784364298934\n\n\n\n\nCode\nbest_model_SVR \n\n\nSVR(C=0.1, gamma=0.0001, kernel='poly')\n\n\nCode\npredict_test_SVR = best_model_SVR.predict(X_test_scaled)\n\nSVR_test_rmsle = mean_squared_log_error(Y_test, predict_test_SVR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", SVR_test_rmsle)\n\n\nTest error (RMSLE): 1.0137954511265557\n\n\n\n\nCode\nbest_model_ridge\n\n\nRidge(alpha=100.0)\n\n\nCode\npredict_test_ridge = best_model_ridge.predict(X_test_scaled)\n\n\nridge_test_rmsle = mean_squared_log_error(Y_test, predict_test_ridge,squared=False)\n\n\nprint(\"Test error (RMSLE):\", ridge_test_rmsle)\n\n\nTest error (RMSLE): 0.5376660192539188\n\n\n\n\nCode\n\nbest_model_GBR\n\n\nGradientBoostingRegressor(learning_rate=0.01, max_depth=6)\n\n\nCode\npredict_test_GBR = best_model_GBR.predict(X_test)\n\n\nGBR_test_rmsle = mean_squared_log_error(Y_test, predict_test_GBR,squared=False)\n\n\nprint(\"Test error (RMSLE):\", GBR_test_rmsle)\n\n\nTest error (RMSLE): 0.7875342011788629"
  },
  {
    "objectID": "templates/AboutTemplate.html",
    "href": "templates/AboutTemplate.html",
    "title": "Your Name",
    "section": "",
    "text": "##Instructions"
  },
  {
    "objectID": "templates/AboutTemplate.html#educationwork-background",
    "href": "templates/AboutTemplate.html#educationwork-background",
    "title": "Your Name",
    "section": "Education/Work Background",
    "text": "Education/Work Background"
  },
  {
    "objectID": "templates/AboutTemplate.html#r-experience",
    "href": "templates/AboutTemplate.html#r-experience",
    "title": "Your Name",
    "section": "R experience",
    "text": "R experience"
  },
  {
    "objectID": "templates/AboutTemplate.html#research-interests",
    "href": "templates/AboutTemplate.html#research-interests",
    "title": "Your Name",
    "section": "Research interests",
    "text": "Research interests"
  },
  {
    "objectID": "templates/AboutTemplate.html#hometown",
    "href": "templates/AboutTemplate.html#hometown",
    "title": "Your Name",
    "section": "Hometown",
    "text": "Hometown"
  },
  {
    "objectID": "templates/AboutTemplate.html#hobbies",
    "href": "templates/AboutTemplate.html#hobbies",
    "title": "Your Name",
    "section": "Hobbies",
    "text": "Hobbies"
  },
  {
    "objectID": "templates/AboutTemplate.html#fun-fact",
    "href": "templates/AboutTemplate.html#fun-fact",
    "title": "Your Name",
    "section": "Fun fact",
    "text": "Fun fact"
  },
  {
    "objectID": "templates/PostTemplate.html",
    "href": "templates/PostTemplate.html",
    "title": "Blog Post Template",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE)"
  },
  {
    "objectID": "templates/PostTemplate.html#instructions",
    "href": "templates/PostTemplate.html#instructions",
    "title": "Blog Post Template",
    "section": "Instructions",
    "text": "Instructions\nThis document provides yaml header inforamtion you will need to replicate each week to submit your homework or other blog posts. Please observe the following conventions:\n\nSave your own copy of this template as a blog post in the posts folder.\nEdit the yaml header to change your author name\ninclude a description that is reader friendly\nupdate the category list to indicate what category is the blog post, the data used, the main packages or techniques, your name, or any thing else to make your document easy to find\nedit as a normal qmd/rmd file\n\n\n\nCode\nx <- c(2,3,4,5)\nmean(x)\n\n\n[1] 3.5"
  },
  {
    "objectID": "templates/PostTemplate.html#rendering-your-post",
    "href": "templates/PostTemplate.html#rendering-your-post",
    "title": "Blog Post Template",
    "section": "Rendering your post",
    "text": "Rendering your post\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure that you have moved your *.qmd file into the posts folder BEFORE you render it, so that all files are stored in the correct location.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOnly render a single file - don’t try to render the whole website!\n\n\n\n\n\n\n\n\nPilot Student Blogs\n\n\n\nWe are piloting a workflow including individual student websites with direted and limited pull requests back to course blogs. Please let us know if you would like to participate."
  },
  {
    "objectID": "templates/PostTemplate.html#reading-in-data-files",
    "href": "templates/PostTemplate.html#reading-in-data-files",
    "title": "Blog Post Template",
    "section": "Reading in data files",
    "text": "Reading in data files\nThe easiest data source to use - at least initially - is to choose something easily accessible, either from our _data folder provided, or from an online source that is publicly available.\n\n\n\n\n\n\nUsing Other Data\n\n\n\nIf you would like to use a source that you have access to and it is small enough and you don’t mind making it public, you can copy it into the _data file and include in your commit and pull request.\n\n\n\n\n\n\n\n\nUsing Private Data\n\n\n\nIf you would like to use a proprietary source of data, that should be possible using the same process outlined above. There may initially be a few issues. We hope to have this feature working smoothly soon!"
  }
]